{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5Glay1esC4IO"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["![download (8).svg](data:image/svg+xml;base64,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)"],"metadata":{"id":"zBLwPlzL2FsT"}},{"cell_type":"markdown","source":["![download (9).svg](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAwMCIgaGVpZ2h0PSIzMDAiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+Cgo8c3R5bGU+CnRleHQgeyBmb250LWZhbWlseTogQXJpYWwsIHNhbnMtc2VyaWY7IGZvbnQtc2l6ZToxNHB4OyB9Ci5ibG9jayB7IHN0cm9rZTojMzMzOyBzdHJva2Utd2lkdGg6MS41OyByeDo4OyByeTo4OyB9Ci5hcnJvdyB7IHN0cm9rZTojMzMzOyBzdHJva2Utd2lkdGg6MS42OyBmaWxsOm5vbmU7IH0KPC9zdHlsZT4KCjxkZWZzPgo8bWFya2VyIGlkPSJhcnJvdyIgbWFya2VyV2lkdGg9IjEwIiBtYXJrZXJIZWlnaHQ9IjEwIiByZWZYPSI5IiByZWZZPSIzIiBvcmllbnQ9ImF1dG8iPgogIDxwYXRoIGQ9Ik0wLDAgTDAsNiBMOSwzIHoiIGZpbGw9IiMzMzMiPjwvcGF0aD4KPC9tYXJrZXI+CjwvZGVmcz4KCjwhLS0gSW5wdXQgLS0+CjxyZWN0IGNsYXNzPSJibG9jayIgeD0iNDAiIHk9IjEyMCIgd2lkdGg9IjI0MCIgaGVpZ2h0PSI2MCIgZmlsbD0iI2QxYzRlOSI+PC9yZWN0Pgo8dGV4dCB4PSIxNjAiIHk9IjE1NSIgdGV4dC1hbmNob3I9Im1pZGRsZSI+CkxhdGVudCBFbWJlZGRpbmcgeiAodjMpCjwvdGV4dD4KCjwhLS0gRkMgLS0+CjxyZWN0IGNsYXNzPSJibG9jayIgeD0iMzYwIiB5PSIxMjAiIHdpZHRoPSIyNjAiIGhlaWdodD0iNjAiIGZpbGw9IiNmZmUwYjIiPjwvcmVjdD4KPHRleHQgeD0iNDkwIiB5PSIxNDUiIHRleHQtYW5jaG9yPSJtaWRkbGUiPgpGdWxseSBDb25uZWN0ZWQKPC90ZXh0Pgo8dGV4dCB4PSI0OTAiIHk9IjE2NSIgdGV4dC1hbmNob3I9Im1pZGRsZSI+ClJlTFUgKyBEcm9wb3V0CjwvdGV4dD4KCjwhLS0gT3V0cHV0IC0tPgo8cmVjdCBjbGFzcz0iYmxvY2siIHg9IjcwMCIgeT0iMTIwIiB3aWR0aD0iMjYwIiBoZWlnaHQ9IjYwIiBmaWxsPSIjYTVkNmE3Ij48L3JlY3Q+Cjx0ZXh0IHg9IjgzMCIgeT0iMTU1IiB0ZXh0LWFuY2hvcj0ibWlkZGxlIj4KRHJpdmVyIElkZW50aXR5CjwvdGV4dD4KCjwhLS0gQXJyb3dzIC0tPgo8bGluZSBjbGFzcz0iYXJyb3ciIHgxPSIyODAiIHkxPSIxNTAiIHgyPSIzNjAiIHkyPSIxNTAiIG1hcmtlci1lbmQ9InVybCgjYXJyb3cpIj48L2xpbmU+CjxsaW5lIGNsYXNzPSJhcnJvdyIgeDE9IjYyMCIgeTE9IjE1MCIgeDI9IjcwMCIgeTI9IjE1MCIgbWFya2VyLWVuZD0idXJsKCNhcnJvdykiPjwvbGluZT4KCjwvc3ZnPgo=)"],"metadata":{"id":"upQbTc9W2MHI"}},{"cell_type":"markdown","source":["![download (11).svg](data:image/svg+xml;base64,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)"],"metadata":{"id":"po4OQGq32MU1"}},{"cell_type":"markdown","source":["<style>\n","table{border-collapse:collapse;width:100%;font-family:Arial}\n","th,td{border:1px solid #333;padding:8px}\n","th{background:#0b3c8a;color:#fff}\n",".v1{background:#e3f2fd}\n",".v2{background:#e8f5e9}\n",".v3{background:#fff3e0}\n",".v4{background:#fce4ec}\n","</style>\n","\n","<table>\n","<tr>\n","<th>Aspect</th><th>v1</th><th>v2</th><th>v3</th><th>v4</th>\n","</tr>\n","<tr>\n","<td>Latent Design</td>\n","<td class=\"v1\">Single z</td>\n","<td class=\"v2\">Œ≤-VAE</td>\n","<td class=\"v3\">Contrastive</td>\n","<td class=\"v4\">Hierarchical z‚ÇÅ+z‚ÇÇ</td>\n","</tr>\n","<tr>\n","<td>Temporal Awareness</td>\n","<td class=\"v1\">No</td>\n","<td class=\"v2\">No</td>\n","<td class=\"v3\">Implicit</td>\n","<td class=\"v4\">Explicit Regularization</td>\n","</tr>\n","<tr>\n","<td>Separability</td>\n","<td class=\"v1\">Moderate</td>\n","<td class=\"v2\">Improved</td>\n","<td class=\"v3\">Strong</td>\n","<td class=\"v4\">Very Strong</td>\n","</tr>\n","<tr>\n","<td>Edge Readiness</td>\n","<td class=\"v1\">Prototype</td>\n","<td class=\"v2\">Near-Edge</td>\n","<td class=\"v3\">Edge</td>\n","<td class=\"v4\">Edge-Optimized</td>\n","</tr>\n","</table>\n"],"metadata":{"id":"tJyo_nLQ2Mbb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pD6g2YGC4Lv"},"outputs":[],"source":["# ============================================================\n","# üöó SIGNet-V3 + IDInferNet + Synthetic Classifier\n","# WITH EARLY STOPPING + FULL ABLATION LOGGING\n","# ============================================================\n","\n","!pip -q install torch torchvision torchaudio seaborn scikit-learn thop openpyxl\n","\n","import os, time, copy\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.decomposition import PCA\n","from scipy.spatial.distance import euclidean\n","from thop import profile\n","\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# ============================================================\n","# 1. DATA & DIRECTORIES\n","# ============================================================\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","DATA_DIR = \"/content/drive/MyDrive/DT_Driver_Wise_Data\"\n","RES_DIR = f\"{DATA_DIR}/SIGNetv3_Results\"\n","FIG_DIR = f\"{RES_DIR}/Figures\"\n","EXCEL_DIR = f\"{RES_DIR}/Excel\"\n","MODEL_DIR = f\"{RES_DIR}/Models\"\n","\n","for d in [RES_DIR, FIG_DIR, EXCEL_DIR, MODEL_DIR]:\n","    os.makedirs(d, exist_ok=True)\n","\n","DRIVERS = [\"B\", \"D\", \"F\"]\n","FEATURES = [\n","    \"Long_Term_Fuel_Trim_Bank1\",\n","    \"Engine_coolant_temperature.1\",\n","    \"Activation_of_Air_compressor\",\n","    \"Torque_of_friction\",\n","    \"Engine_soacking_time\",\n","    \"Intake_air_pressure\",\n","]\n","TIME_COL = \"Time(s)\"\n","\n","dfs = []\n","for d in DRIVERS:\n","    for s in [\"Train\", \"Valid\"]:\n","        p = f\"{DATA_DIR}/Driver_{d}_{s}.csv\"\n","        if os.path.exists(p):\n","            df = pd.read_csv(p)\n","            df[\"Driver\"] = d\n","            df[\"Split\"] = s\n","            dfs.append(df)\n","\n","df = pd.concat(dfs, ignore_index=True).dropna(subset=FEATURES + [TIME_COL])\n","\n","# ============================================================\n","# 2. PREPROCESSING\n","# ============================================================\n","sc_x, sc_t = StandardScaler(), StandardScaler()\n","enc = OneHotEncoder(sparse_output=False)\n","\n","X = sc_x.fit_transform(df[FEATURES])\n","T = sc_t.fit_transform(df[[TIME_COL]])\n","D = enc.fit_transform(df[[\"Driver\"]])\n","C = np.concatenate([T, D], axis=1)\n","\n","X = torch.tensor(X, dtype=torch.float32).to(device)\n","C = torch.tensor(C, dtype=torch.float32).to(device)\n","\n","mask_tr = df[\"Split\"] == \"Train\"\n","mask_va = df[\"Split\"] == \"Valid\"\n","\n","X_tr, C_tr = X[mask_tr], C[mask_tr]\n","X_va, C_va = X[mask_va], C[mask_va]\n","drv_va = df.loc[mask_va, \"Driver\"].values\n","\n","le = LabelEncoder()\n","driver_ids = torch.tensor(le.fit_transform(df[\"Driver\"]), dtype=torch.long).to(device)\n","\n","# ============================================================\n","# 3. SIGNet-V3 MODEL\n","# ============================================================\n","class SIGNet(nn.Module):\n","    def __init__(self, xdim, cdim, zdim=256, h=256):\n","        super().__init__()\n","        self.enc = nn.Sequential(\n","            nn.Linear(xdim + cdim, h), nn.ReLU(),\n","            nn.Linear(h, h), nn.ReLU()\n","        )\n","        self.mu = nn.Linear(h, zdim)\n","        self.logvar = nn.Linear(h, zdim)\n","        self.dec_fc = nn.Linear(zdim + cdim, h)\n","        self.dec = nn.Sequential(nn.ReLU(), nn.Linear(h, xdim))\n","\n","    def encode(self, x, c):\n","        h = self.enc(torch.cat([x, c], 1))\n","        return self.mu(h), self.logvar(h)\n","\n","    def reparam(self, mu, lv):\n","        return mu + torch.randn_like(mu) * torch.exp(0.5 * lv)\n","\n","    def decode(self, z, c):\n","        return self.dec(self.dec_fc(torch.cat([z, c], 1)))\n","\n","    def forward(self, x, c):\n","        mu, lv = self.encode(x, c)\n","        z = self.reparam(mu, lv)\n","        return self.decode(z, c), mu, lv\n","\n","def vae_loss(r, x, mu, lv):\n","    rec = F.mse_loss(r, x)\n","    kl = -0.5 * torch.mean(1 + lv - mu**2 - lv.exp())\n","    return rec + kl, rec, kl\n","\n","def contrastive(mu, labels, margin=1.0):\n","    d = torch.cdist(mu, mu)\n","    same = (labels[:, None] == labels[None, :]).float()\n","    diff = 1 - same\n","    return (same * d**2).mean() + (diff * F.relu(margin - d)**2).mean()\n","\n","# ============================================================\n","# 4. TRAIN BASELINE & OPTIMIZED (WITH EARLY STOPPING)\n","# ============================================================\n","history = {}\n","early_meta = {}\n","\n","for mode in [\"baseline\", \"optimized\"]:\n","\n","    model = SIGNet(X_tr.shape[1], C_tr.shape[1]).to(device)\n","    opt = optim.Adam(model.parameters(), lr=1e-3 if mode == \"baseline\" else 1e-4)\n","\n","    best_val = np.inf\n","    best_epoch = 0\n","    patience = 20\n","    delta = 1e-4\n","    no_improve = 0\n","\n","    tr_hist, va_hist = [], []\n","\n","    for ep in range(1, 401):\n","        model.train()\n","        opt.zero_grad()\n","\n","        r, mu, lv = model(X_tr, C_tr)\n","        base, _, _ = vae_loss(r, X_tr, mu, lv)\n","        loss = base if mode == \"baseline\" else base + 0.1 * contrastive(mu, driver_ids[mask_tr])\n","\n","        loss.backward()\n","        opt.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            rv, muv, lvv = model(X_va, C_va)\n","            vloss, _, _ = vae_loss(rv, X_va, muv, lvv)\n","\n","        tr_hist.append(loss.item())\n","        va_hist.append(vloss.item())\n","\n","        if vloss < best_val - delta:\n","            best_val = vloss\n","            best_epoch = ep\n","            best_wts = copy.deepcopy(model.state_dict())\n","            no_improve = 0\n","        else:\n","            no_improve += 1\n","\n","        if ep % 25 == 0:\n","            print(f\"{mode.upper()} | Ep {ep:03d} | Train={loss:.4f} | Val={vloss:.4f}\")\n","\n","        if no_improve >= patience:\n","            print(f\"‚èπÔ∏è {mode.upper()} early stopped at epoch {ep}\")\n","            break\n","\n","    model.load_state_dict(best_wts)\n","    torch.save(model.state_dict(), f\"{MODEL_DIR}/signet_v3_{mode}.pt\")\n","\n","    history[mode] = {\"train\": tr_hist, \"val\": va_hist}\n","    early_meta[mode] = {\n","        \"Best_Val_Loss\": best_val,\n","        \"Best_Epoch\": best_epoch,\n","        \"Stopped_Epoch\": ep\n","    }\n","\n","# ============================================================\n","# 5. DRIVER SIGNATURES\n","# ============================================================\n","model.load_state_dict(torch.load(f\"{MODEL_DIR}/signet_v3_optimized.pt\"))\n","model.eval()\n","\n","signatures = {}\n","with torch.no_grad():\n","    for d in DRIVERS:\n","        m = df[\"Driver\"] == d\n","        mu, _ = model.encode(X[m], C[m])\n","        signatures[d] = mu.mean(0).cpu().numpy()\n","\n","# ============================================================\n","# 6. IDINFERNET (LATENT CLASSIFIER + EARLY STOPPING)\n","# ============================================================\n","class IDInferNet(nn.Module):\n","    def __init__(self, z, n):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(z, 128), nn.ReLU(),\n","            nn.Linear(128, n)\n","        )\n","    def forward(self, z): return self.net(z)\n","\n","with torch.no_grad():\n","    Z_va = model.encode(X_va, C_va)[0]\n","\n","y_va = torch.tensor(le.transform(drv_va), dtype=torch.long).to(device)\n","\n","idnet = IDInferNet(Z_va.shape[1], len(DRIVERS)).to(device)\n","opt_id = optim.Adam(idnet.parameters(), lr=1e-3)\n","\n","best_acc = 0\n","patience = 10\n","no_improve = 0\n","\n","for ep in range(1, 101):\n","    opt_id.zero_grad()\n","    logits = idnet(Z_va)\n","    loss = F.cross_entropy(logits, y_va)\n","    loss.backward()\n","    opt_id.step()\n","\n","    acc = accuracy_score(y_va.cpu(), logits.argmax(1).cpu())\n","    if acc > best_acc:\n","        best_acc = acc\n","        best_state = copy.deepcopy(idnet.state_dict())\n","        no_improve = 0\n","    else:\n","        no_improve += 1\n","\n","    if no_improve >= patience:\n","        print(f\"‚èπÔ∏è IDInferNet early stopped at epoch {ep}\")\n","        break\n","\n","idnet.load_state_dict(best_state)\n","\n","# ============================================================\n","# 7. SYNTHETIC DATA CLASSIFIER (EARLY STOPPING)\n","# ============================================================\n","Xs, ys = [], []\n","for d in DRIVERS:\n","    mu = torch.tensor(signatures[d], device=device)\n","    Cd = C[df[\"Driver\"] == d]\n","    for _ in range(2000):\n","        z = mu + torch.randn_like(mu) * 0.8\n","        c = Cd[np.random.randint(0, len(Cd))]\n","        x = model.decode(z.unsqueeze(0), c.unsqueeze(0))\n","        Xs.append(x.cpu().numpy()[0])\n","        ys.append(le.transform([d])[0])\n","\n","Xs, ys = np.array(Xs), np.array(ys)\n","Xs_tr, Xs_va, ys_tr, ys_va = train_test_split(Xs, ys, stratify=ys, test_size=0.3)\n","\n","class DriverClassifier(nn.Module):\n","    def __init__(self, d, n):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(d, 256), nn.ReLU(),\n","            nn.Linear(256, n)\n","        )\n","    def forward(self, x): return self.net(x)\n","\n","clf = DriverClassifier(Xs_tr.shape[1], len(DRIVERS)).to(device)\n","opt_c = optim.Adam(clf.parameters(), lr=1e-3)\n","\n","Xt = torch.tensor(Xs_tr, dtype=torch.float32).to(device)\n","yt = torch.tensor(ys_tr, dtype=torch.long).to(device)\n","Xv = torch.tensor(Xs_va, dtype=torch.float32).to(device)\n","yv = torch.tensor(ys_va, dtype=torch.long).to(device)\n","\n","best_val = np.inf\n","no_improve = 0\n","for ep in range(1, 201):\n","    opt_c.zero_grad()\n","    loss = F.cross_entropy(clf(Xt), yt)\n","    loss.backward()\n","    opt_c.step()\n","\n","    with torch.no_grad():\n","        vloss = F.cross_entropy(clf(Xv), yv)\n","\n","    if vloss < best_val - 1e-4:\n","        best_val = vloss\n","        best_state = copy.deepcopy(clf.state_dict())\n","        no_improve = 0\n","    else:\n","        no_improve += 1\n","\n","    if no_improve >= 15:\n","        print(f\"‚èπÔ∏è Synthetic classifier early stopped at epoch {ep}\")\n","        break\n","\n","clf.load_state_dict(best_state)\n","\n","# ============================================================\n","# 8. SAVE ABLATION FIGURE\n","# ============================================================\n","plt.figure()\n","plt.plot(history[\"baseline\"][\"val\"], label=\"Baseline\")\n","plt.plot(history[\"optimized\"][\"val\"], label=\"Optimized\")\n","plt.legend()\n","plt.title(\"SIGNet-V3 Ablation (Validation Loss)\")\n","plt.savefig(f\"{FIG_DIR}/ablation_val_loss.png\", dpi=300)\n","plt.close()\n","\n","# ============================================================\n","# 9. SAVE EXCEL (FULL METADATA)\n","# ============================================================\n","excel_path = f\"{EXCEL_DIR}/SIGNetV3_Full_Results.xlsx\"\n","with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as w:\n","    pd.DataFrame(history[\"baseline\"]).to_excel(w, \"Baseline_Loss\")\n","    pd.DataFrame(history[\"optimized\"]).to_excel(w, \"Optimized_Loss\")\n","    pd.DataFrame(early_meta).T.to_excel(w, \"Early_Stopping_Summary\")\n","    pd.DataFrame(signatures).T.to_excel(w, \"Driver_Signatures\")\n","    pd.DataFrame(classification_report(\n","        y_va.cpu(), idnet(Z_va).argmax(1).cpu(), output_dict=True\n","    )).T.to_excel(w, \"IDInferNet_Report\")\n","\n","print(f\"\\n‚úÖ ALL RESULTS SAVED TO:\\n{excel_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzrbMxEyFszz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G72pe-kFhiJ3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"leT-8TFihiM_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"erlRCwOrhiPz"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"pbGIGlTwhrW5"},"source":["<style>\n",".comp-table {\n","  border-collapse: collapse;\n","  width: 100%;\n","  font-family: Arial, sans-serif;\n","  font-size: 14px;\n","}\n",".comp-table th {\n","  background-color: #0b3c8a;\n","  color: white;\n","  padding: 10px;\n","  border: 1px solid #333;\n","}\n",".comp-table td {\n","  padding: 10px;\n","  border: 1px solid #333;\n","}\n",".comp-table tr:nth-child(even) {\n","  background-color: #f2f4f8;\n","}\n",".v1 { background:#e3f2fd; }\n",".v2 { background:#e8f5e9; }\n",".v3 { background:#fff3e0; }\n","</style>\n","\n","<table class=\"comp-table\">\n","<tr>\n","<th>Aspect</th>\n","<th>SIGNet-v1</th>\n","<th>SIGNet-v2</th>\n","<th>SIGNet-v3 (Proposed)</th>\n","</tr>\n","\n","<tr>\n","<td>Encoder Depth</td>\n","<td class=\"v1\">Shallow</td>\n","<td class=\"v2\">Deeper + BatchNorm</td>\n","<td class=\"v3\">Deep + Contrastive-aware</td>\n","</tr>\n","\n","<tr>\n","<td>Latent Dimension</td>\n","<td class=\"v1\">Low (8)</td>\n","<td class=\"v2\">Medium (16)</td>\n","<td class=\"v3\">High (32)</td>\n","</tr>\n","\n","<tr>\n","<td>Loss Function</td>\n","<td class=\"v1\">VAE</td>\n","<td class=\"v2\">Œ≤-VAE</td>\n","<td class=\"v3\">Œ≤-VAE + Contrastive</td>\n","</tr>\n","\n","<tr>\n","<td>Latent Separability</td>\n","<td class=\"v1\">Moderate</td>\n","<td class=\"v2\">Improved</td>\n","<td class=\"v3\">Strong (cluster-aware)</td>\n","</tr>\n","\n","<tr>\n","<td>Digital Twin Quality</td>\n","<td class=\"v1\">Basic</td>\n","<td class=\"v2\">Stable</td>\n","<td class=\"v3\">Discriminative</td>\n","</tr>\n","\n","<tr>\n","<td>DT-GDIN Robustness</td>\n","<td class=\"v1\">Limited</td>\n","<td class=\"v2\">Improved</td>\n","<td class=\"v3\">High</td>\n","</tr>\n","\n","<tr>\n","<td>Edge Readiness</td>\n","<td class=\"v1\">Prototype</td>\n","<td class=\"v2\">Near-edge</td>\n","<td class=\"v3\">Edge-deployable</td>\n","</tr>\n","\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"swPaKYzgi4uR"},"source":["![download (1).svg](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQwMCIgaGVpZ2h0PSI0ODAiIHZpZXdCb3g9IjAgMCAxNDAwIDQ4MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCjxzdHlsZT4KICB0ZXh0IHsgZm9udC1mYW1pbHk6IEFyaWFsLCBzYW5zLXNlcmlmOyB9CiAgLnRpdGxlIHsgZm9udC1zaXplOjE4cHg7IGZvbnQtd2VpZ2h0OmJvbGQ7IH0KICAuYmxvY2sgeyBzdHJva2U6IzMzMzsgc3Ryb2tlLXdpZHRoOjEuNTsgcng6Njsgcnk6NjsgfQogIC5sYWJlbCB7IGZvbnQtc2l6ZToxNHB4OyB0ZXh0LWFuY2hvcjptaWRkbGU7IGRvbWluYW50LWJhc2VsaW5lOm1pZGRsZTsgfQogIC5zdWIgeyBmb250LXNpemU6MTJweDsgdGV4dC1hbmNob3I6bWlkZGxlOyBmaWxsOiMyMjI7IH0KICAuYXJyb3cgeyBzdHJva2U6IzMzMzsgc3Ryb2tlLXdpZHRoOjEuNjsgZmlsbDpub25lOyB9CiAgLmxvc3MgeyBzdHJva2U6I2M2MjgyODsgc3Ryb2tlLXdpZHRoOjEuODsgc3Ryb2tlLWRhc2hhcnJheTo1LDQ7IH0KPC9zdHlsZT4KCjxkZWZzPgogIDxtYXJrZXIgaWQ9ImFycm93IiBtYXJrZXJXaWR0aD0iMTAiIG1hcmtlckhlaWdodD0iMTAiIHJlZlg9IjkiIHJlZlk9IjMiIG9yaWVudD0iYXV0byI+CiAgICA8cGF0aCBkPSJNMCwwIEwwLDYgTDksMyB6IiBmaWxsPSIjMzMzIj48L3BhdGg+CiAgPC9tYXJrZXI+CjwvZGVmcz4KCjx0ZXh0IHg9IjIwIiB5PSIzMCIgY2xhc3M9InRpdGxlIj4KU0lHTmV0LXYzIEFyY2hpdGVjdHVyZSAoQ29udHJhc3RpdmUtUmVndWxhcml6ZWQgRGlnaXRhbCBUd2luKQo8L3RleHQ+Cgo8IS0tIElucHV0cyAtLT4KPHJlY3QgY2xhc3M9ImJsb2NrIiB4PSI0MCIgeT0iMTgwIiB3aWR0aD0iMjQwIiBoZWlnaHQ9IjYwIiBmaWxsPSIjYjNlNWZjIj48L3JlY3Q+Cjx0ZXh0IGNsYXNzPSJsYWJlbCIgeD0iMTYwIiB5PSIyMTAiPlNlbnNvciArIENvbnRleHQ8L3RleHQ+Cgo8IS0tIEVuY29kZXIgLS0+CjxyZWN0IGNsYXNzPSJibG9jayIgeD0iMzQwIiB5PSIxNjAiIHdpZHRoPSIzMDAiIGhlaWdodD0iMTAwIiBmaWxsPSIjOTBjYWY5Ij48L3JlY3Q+Cjx0ZXh0IGNsYXNzPSJsYWJlbCIgeD0iNDkwIiB5PSIxODUiPkRlZXAgRW5jb2RlcjwvdGV4dD4KPHRleHQgY2xhc3M9InN1YiIgeD0iNDkwIiB5PSIyMDUiPkZDIOKGkiBSZUxVPC90ZXh0Pgo8dGV4dCBjbGFzcz0ic3ViIiB4PSI0OTAiIHk9IjIyNSI+RkMg4oaSIFJlTFU8L3RleHQ+Cjx0ZXh0IGNsYXNzPSJzdWIiIHg9IjQ5MCIgeT0iMjQ1Ij5IaWdoLUNhcGFjaXR5IE1hcHBpbmc8L3RleHQ+Cgo8IS0tIExhdGVudCAtLT4KPHJlY3QgY2xhc3M9ImJsb2NrIiB4PSI3MjAiIHk9IjE0MCIgd2lkdGg9IjIyMCIgaGVpZ2h0PSI2MCIgZmlsbD0iI2QxYzRlOSI+PC9yZWN0Pgo8dGV4dCBjbGFzcz0ibGFiZWwiIHg9IjgzMCIgeT0iMTcwIj7OvCAsIM+DwrI8L3RleHQ+Cgo8cmVjdCBjbGFzcz0iYmxvY2siIHg9IjcyMCIgeT0iMjQwIiB3aWR0aD0iMjIwIiBoZWlnaHQ9IjYwIiBmaWxsPSIjY2U5M2Q4Ij48L3JlY3Q+Cjx0ZXh0IGNsYXNzPSJsYWJlbCIgeD0iODMwIiB5PSIyNzAiPkxhdGVudCB6PC90ZXh0PgoKPCEtLSBDb250cmFzdGl2ZSAtLT4KPHJlY3QgY2xhc3M9ImJsb2NrIiB4PSI3MjAiIHk9IjMzMCIgd2lkdGg9IjIyMCIgaGVpZ2h0PSI2MCIgZmlsbD0iI2ZmY2RkMiI+PC9yZWN0Pgo8dGV4dCBjbGFzcz0ibGFiZWwiIHg9IjgzMCIgeT0iMzYwIj5Db250cmFzdGl2ZSBMb3NzPC90ZXh0PgoKPCEtLSBEZWNvZGVyIC0tPgo8cmVjdCBjbGFzcz0iYmxvY2siIHg9IjEwMjAiIHk9IjE2MCIgd2lkdGg9IjMwMCIgaGVpZ2h0PSIxMDAiIGZpbGw9IiNjOGU2YzkiPjwvcmVjdD4KPHRleHQgY2xhc3M9ImxhYmVsIiB4PSIxMTcwIiB5PSIxODUiPkRlY29kZXI8L3RleHQ+Cjx0ZXh0IGNsYXNzPSJzdWIiIHg9IjExNzAiIHk9IjIwNSI+RkMg4oaSIFJlTFU8L3RleHQ+Cjx0ZXh0IGNsYXNzPSJzdWIiIHg9IjExNzAiIHk9IjIyNSI+RkMg4oaSIFJlY29uc3RydWN0aW9uPC90ZXh0PgoKPCEtLSBPdXRwdXQgLS0+CjxyZWN0IGNsYXNzPSJibG9jayIgeD0iMTM0MCIgeT0iMTkwIiB3aWR0aD0iNDAiIGhlaWdodD0iNjAiIGZpbGw9IiNhNWQ2YTciPjwvcmVjdD4KPHRleHQgY2xhc3M9ImxhYmVsIiB4PSIxMzYwIiB5PSIyMjAiPnjMgjwvdGV4dD4KCjwhLS0gQXJyb3dzIC0tPgo8bGluZSBjbGFzcz0iYXJyb3ciIHgxPSIyODAiIHkxPSIyMTAiIHgyPSIzNDAiIHkyPSIyMTAiIG1hcmtlci1lbmQ9InVybCgjYXJyb3cpIj48L2xpbmU+CjxsaW5lIGNsYXNzPSJhcnJvdyIgeDE9IjY0MCIgeTE9IjIxMCIgeDI9IjcyMCIgeTI9IjE3MCIgbWFya2VyLWVuZD0idXJsKCNhcnJvdykiPjwvbGluZT4KPGxpbmUgY2xhc3M9ImFycm93IiB4MT0iODMwIiB5MT0iMjAwIiB4Mj0iODMwIiB5Mj0iMjQwIiBtYXJrZXItZW5kPSJ1cmwoI2Fycm93KSI+PC9saW5lPgo8bGluZSBjbGFzcz0iYXJyb3ciIHgxPSI5NDAiIHkxPSIyNzAiIHgyPSIxMDIwIiB5Mj0iMjEwIiBtYXJrZXItZW5kPSJ1cmwoI2Fycm93KSI+PC9saW5lPgo8bGluZSBjbGFzcz0iYXJyb3ciIHgxPSIxMzIwIiB5MT0iMjIwIiB4Mj0iMTM0MCIgeTI9IjIyMCIgbWFya2VyLWVuZD0idXJsKCNhcnJvdykiPjwvbGluZT4KCjwhLS0gQ29udHJhc3RpdmUgZmVlZGJhY2sgLS0+CjxsaW5lIGNsYXNzPSJsb3NzIiB4MT0iODMwIiB5MT0iMzAwIiB4Mj0iODMwIiB5Mj0iMzMwIiBtYXJrZXItZW5kPSJ1cmwoI2Fycm93KSI+PC9saW5lPgo8bGluZSBjbGFzcz0ibG9zcyIgeDE9IjcyMCIgeTE9IjM2MCIgeDI9IjQ5MCIgeTI9IjI2MCIgbWFya2VyLWVuZD0idXJsKCNhcnJvdykiPjwvbGluZT4KCjwvc3ZnPgo=)"]},{"cell_type":"markdown","metadata":{"id":"UPT9U0RulRJ7"},"source":["![download (5).svg](data:image/svg+xml;base64,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)"]},{"cell_type":"markdown","metadata":{"id":"c_awpZGMlzKD"},"source":["IDInferNet-v3 Architecture\n","\n","(Discriminative Latent Classifier ‚Äì Real Data)\n","\n","![download (6).svg](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAwMCIgaGVpZ2h0PSIzMDAiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+Cgo8c3R5bGU+CnRleHQgeyBmb250LWZhbWlseTogQXJpYWwsIHNhbnMtc2VyaWY7IGZvbnQtc2l6ZToxNHB4OyB9Ci5ibG9jayB7IHN0cm9rZTojMzMzOyBzdHJva2Utd2lkdGg6MS41OyByeDo4OyByeTo4OyB9Ci5hcnJvdyB7IHN0cm9rZTojMzMzOyBzdHJva2Utd2lkdGg6MS42OyBmaWxsOm5vbmU7IH0KPC9zdHlsZT4KCjxkZWZzPgo8bWFya2VyIGlkPSJhcnJvdyIgbWFya2VyV2lkdGg9IjEwIiBtYXJrZXJIZWlnaHQ9IjEwIiByZWZYPSI5IiByZWZZPSIzIiBvcmllbnQ9ImF1dG8iPgogIDxwYXRoIGQ9Ik0wLDAgTDAsNiBMOSwzIHoiIGZpbGw9IiMzMzMiPjwvcGF0aD4KPC9tYXJrZXI+CjwvZGVmcz4KCjwhLS0gSW5wdXQgLS0+CjxyZWN0IGNsYXNzPSJibG9jayIgeD0iNDAiIHk9IjEyMCIgd2lkdGg9IjI0MCIgaGVpZ2h0PSI2MCIgZmlsbD0iI2QxYzRlOSI+PC9yZWN0Pgo8dGV4dCB4PSIxNjAiIHk9IjE1NSIgdGV4dC1hbmNob3I9Im1pZGRsZSI+CkxhdGVudCBFbWJlZGRpbmcgeiAodjMpCjwvdGV4dD4KCjwhLS0gRkMgLS0+CjxyZWN0IGNsYXNzPSJibG9jayIgeD0iMzYwIiB5PSIxMjAiIHdpZHRoPSIyNjAiIGhlaWdodD0iNjAiIGZpbGw9IiNmZmUwYjIiPjwvcmVjdD4KPHRleHQgeD0iNDkwIiB5PSIxNDUiIHRleHQtYW5jaG9yPSJtaWRkbGUiPgpGdWxseSBDb25uZWN0ZWQKPC90ZXh0Pgo8dGV4dCB4PSI0OTAiIHk9IjE2NSIgdGV4dC1hbmNob3I9Im1pZGRsZSI+ClJlTFUgKyBEcm9wb3V0CjwvdGV4dD4KCjwhLS0gT3V0cHV0IC0tPgo8cmVjdCBjbGFzcz0iYmxvY2siIHg9IjcwMCIgeT0iMTIwIiB3aWR0aD0iMjYwIiBoZWlnaHQ9IjYwIiBmaWxsPSIjYTVkNmE3Ij48L3JlY3Q+Cjx0ZXh0IHg9IjgzMCIgeT0iMTU1IiB0ZXh0LWFuY2hvcj0ibWlkZGxlIj4KRHJpdmVyIElkZW50aXR5CjwvdGV4dD4KCjwhLS0gQXJyb3dzIC0tPgo8bGluZSBjbGFzcz0iYXJyb3ciIHgxPSIyODAiIHkxPSIxNTAiIHgyPSIzNjAiIHkyPSIxNTAiIG1hcmtlci1lbmQ9InVybCgjYXJyb3cpIj48L2xpbmU+CjxsaW5lIGNsYXNzPSJhcnJvdyIgeDE9IjYyMCIgeTE9IjE1MCIgeDI9IjcwMCIgeTI9IjE1MCIgbWFya2VyLWVuZD0idXJsKCNhcnJvdykiPjwvbGluZT4KCjwvc3ZnPgo=)"]},{"cell_type":"markdown","metadata":{"id":"AZ5UVa1Tl4Bd"},"source":["DT-GDIN-v3 Architecture\n","(Digital-Twin-Generated Classifier ‚Äì Synthetic Behavior)\n","![download (7).svg](data:image/svg+xml;base64,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)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XTeCbCrVhiTK","executionInfo":{"status":"ok","timestamp":1765832734831,"user_tz":-300,"elapsed":396306,"user":{"displayName":"Community Of Research & Development","userId":"07614819381775789507"}},"outputId":"16b080c1-e0b8-4cef-af0e-0bde72dc0cbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚öôÔ∏è Using device: cuda\n","Mounted at /content/drive\n","‚úÖ SIGNet-v3 COMPLETE\n","üìÅ Saved to: /content/drive/MyDrive/DT_Driver_Wise_Data/SIGNetV3_Results\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3402535469.py:480: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(hist_base).to_excel(w,\"SIGNet_Base\",index=False)\n","/tmp/ipython-input-3402535469.py:481: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(hist_opt).to_excel(w,\"SIGNet_Optimized\",index=False)\n","/tmp/ipython-input-3402535469.py:483: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  latent_intra_df.to_excel(w,\"Latent_Intra\",index=False)\n","/tmp/ipython-input-3402535469.py:484: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  latent_inter_df.to_excel(w,\"Latent_Inter\",index=False)\n","/tmp/ipython-input-3402535469.py:489: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  }).to_excel(w,\"Cluster_Quality\",index=False)\n","/tmp/ipython-input-3402535469.py:491: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(acc_hist, columns=[\"Accuracy\"]).to_excel(w,\"IDInferNet_Ablation\",index=False)\n","/tmp/ipython-input-3402535469.py:492: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  id_report_df.to_excel(w,\"IDInferNet_Report\")\n","/tmp/ipython-input-3402535469.py:493: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(cm_id).to_excel(w,\"IDInferNet_Confusion\")\n","/tmp/ipython-input-3402535469.py:494: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  id_roc_df.to_excel(w,\"IDInferNet_ROC\")\n","/tmp/ipython-input-3402535469.py:496: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(dt_acc, columns=[\"Accuracy\"]).to_excel(w,\"DT_GDIN_Ablation\",index=False)\n","/tmp/ipython-input-3402535469.py:497: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  dt_report_df.to_excel(w,\"DT_GDIN_Report\")\n","/tmp/ipython-input-3402535469.py:498: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(cm_dt).to_excel(w,\"DT_GDIN_Confusion\")\n","/tmp/ipython-input-3402535469.py:500: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  edge_df.to_excel(w,\"Edge_Metrics\",index=False)\n"]}],"source":["# ============================================================\n","# CVAE_Digital_Twin-v3.ipynb\n","# SIGNet-v3 + IDInferNet-v3 + DT-GDIN-v3\n","# ============================================================\n","\n","!pip -q install torch torchvision torchaudio thop seaborn scikit-learn openpyxl\n","\n","# ============================================================\n","# 1. IMPORTS & CONFIG\n","# ============================================================\n","\n","import os, time, copy\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    accuracy_score, classification_report,\n","    confusion_matrix, roc_curve, auc\n",")\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.metrics import silhouette_score, davies_bouldin_score\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from scipy.spatial.distance import euclidean\n","from thop import profile\n","\n","SEED = 42\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"‚öôÔ∏è Using device: {device}\")\n","\n","# ============================================================\n","# 2. DATA & DIRECTORIES\n","# ============================================================\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","DATA_ROOT = \"/content/drive/MyDrive/DT_Driver_Wise_Data\"\n","RES_ROOT  = f\"{DATA_ROOT}/SIGNetV3_Results\"\n","\n","DIRS = {\n","    \"fig\": f\"{RES_ROOT}/Figures\",\n","    \"mdl\": f\"{RES_ROOT}/Models\",\n","    \"xls\": f\"{RES_ROOT}/Excel\"\n","}\n","for d in DIRS.values():\n","    os.makedirs(d, exist_ok=True)\n","\n","DRIVERS = [\"B\", \"D\", \"F\"]\n","FEATURES = [\n","    \"Long_Term_Fuel_Trim_Bank1\",\n","    \"Engine_coolant_temperature.1\",\n","    \"Activation_of_Air_compressor\",\n","    \"Torque_of_friction\",\n","    \"Engine_soacking_time\",\n","    \"Intake_air_pressure\",\n","]\n","TIME_COL = \"Time(s)\"\n","\n","# ============================================================\n","# 3. LOAD DATA\n","# ============================================================\n","\n","dfs = []\n","for drv in DRIVERS:\n","    for sp in [\"Train\", \"Valid\"]:\n","        p = f\"{DATA_ROOT}/Driver_{drv}_{sp}.csv\"\n","        if os.path.exists(p):\n","            df = pd.read_csv(p)\n","            df[\"Driver\"] = drv\n","            df[\"Split\"] = sp\n","            dfs.append(df)\n","\n","df_all = pd.concat(dfs, ignore_index=True)\n","df_all = df_all.dropna(subset=FEATURES + [TIME_COL])\n","\n","# ============================================================\n","# 4. PREPROCESSING\n","# ============================================================\n","\n","sc_x, sc_t = StandardScaler(), StandardScaler()\n","enc = OneHotEncoder(sparse_output=False)\n","\n","X = sc_x.fit_transform(df_all[FEATURES])\n","T = sc_t.fit_transform(df_all[[TIME_COL]])\n","D = enc.fit_transform(df_all[[\"Driver\"]])\n","C = np.concatenate([T, D], axis=1)\n","\n","X = torch.tensor(X, dtype=torch.float32).to(device)\n","C = torch.tensor(C, dtype=torch.float32).to(device)\n","\n","mask_tr = df_all[\"Split\"] == \"Train\"\n","mask_va = df_all[\"Split\"] == \"Valid\"\n","\n","X_tr, C_tr = X[mask_tr], C[mask_tr]\n","X_va, C_va = X[mask_va], C[mask_va]\n","drv_va     = df_all.loc[mask_va, \"Driver\"].values\n","\n","le_drv = LabelEncoder()\n","drv_id_all = torch.tensor(le_drv.fit_transform(df_all[\"Driver\"]),\n","                          dtype=torch.long).to(device)\n","\n","# ============================================================\n","# 5. SIGNET-v3 (CONTRASTIVE-CVAE)\n","# ============================================================\n","\n","class SIGNetV3(nn.Module):\n","    def __init__(self, x_dim, c_dim, z_dim=32):\n","        super().__init__()\n","        self.enc = nn.Sequential(\n","            nn.Linear(x_dim + c_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 256),\n","            nn.ReLU()\n","        )\n","        self.mu = nn.Linear(256, z_dim)\n","        self.lv = nn.Linear(256, z_dim)\n","\n","        self.dec = nn.Sequential(\n","            nn.Linear(z_dim + c_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, x_dim)\n","        )\n","\n","    def encode(self, x, c):\n","        h = self.enc(torch.cat([x, c], 1))\n","        return self.mu(h), self.lv(h)\n","\n","    def reparam(self, mu, lv):\n","        return mu + torch.randn_like(mu) * torch.exp(0.5 * lv)\n","\n","    def decode(self, z, c):\n","        return self.dec(torch.cat([z, c], 1))\n","\n","    def forward(self, x, c):\n","        mu, lv = self.encode(x, c)\n","        z = self.reparam(mu, lv)\n","        return self.decode(z, c), mu, lv\n","\n","\n","def vae_loss(xr, x, mu, lv, beta=0.5):\n","    rec = F.mse_loss(xr, x)\n","    kl  = -0.5 * torch.mean(1 + lv - mu**2 - lv.exp())\n","    return rec + beta * kl, rec, kl\n","\n","\n","def contrastive_loss(mu, labels, margin=1.0):\n","    d = torch.cdist(mu, mu)\n","    same = (labels[:,None] == labels[None,:]).float()\n","    diff = 1 - same\n","    return (same * d**2).mean() + (diff * F.relu(margin - d)**2).mean()\n","\n","# ============================================================\n","# 6. TRAIN SIGNET-v3 (BASELINE vs OPTIMIZED)\n","# ============================================================\n","\n","def train_signet_v3(use_contrastive, batch_size=256):\n","    model = SIGNetV3(X.shape[1], C.shape[1]).to(device)\n","    opt = optim.Adam(model.parameters(), lr=1e-3)\n","\n","    dataset = torch.utils.data.TensorDataset(\n","        X_tr, C_tr, drv_id_all[mask_tr]\n","    )\n","    loader = torch.utils.data.DataLoader(\n","        dataset, batch_size=batch_size, shuffle=True, drop_last=True\n","    )\n","\n","    best, patience = np.inf, 0\n","    hist = {\"train\": [], \"val\": []}\n","    t0 = time.time()\n","\n","    for ep in range(1, 601):\n","        model.train()\n","        ep_loss = 0.0\n","\n","        for xb, cb, yb in loader:\n","            opt.zero_grad()\n","\n","            xr, mu, lv = model(xb, cb)\n","            base_loss, _, _ = vae_loss(xr, xb, mu, lv)\n","\n","            if use_contrastive:\n","                cl = contrastive_loss(mu, yb)\n","                loss = base_loss + 0.1 * cl\n","            else:\n","                loss = base_loss\n","\n","            loss.backward()\n","            opt.step()\n","            ep_loss += loss.item()\n","\n","        # Validation (unchanged)\n","        model.eval()\n","        with torch.no_grad():\n","            xv, mu_v, lv_v = model(X_va, C_va)\n","            vloss, _, _ = vae_loss(xv, X_va, mu_v, lv_v)\n","\n","        hist[\"train\"].append(ep_loss / len(loader))\n","        hist[\"val\"].append(vloss.item())\n","\n","        if vloss < best - 1e-4:\n","            best = vloss\n","            best_wts = copy.deepcopy(model.state_dict())\n","            patience = 0\n","        else:\n","            patience += 1\n","\n","        if patience >= 40:\n","            break\n","\n","    model.load_state_dict(best_wts)\n","    return model, hist, time.time() - t0\n","\n","\n","\n","model_base, hist_base, time_base = train_signet_v3(False)\n","model_opt,  hist_opt,  time_opt  = train_signet_v3(True)\n","\n","torch.save(model_opt.state_dict(), f\"{DIRS['mdl']}/signet_v3_optimized.pt\")\n","\n","# ============================================================\n","# 7. LATENT EXTRACTION\n","# ============================================================\n","\n","model_opt.eval()\n","with torch.no_grad():\n","    Z_va = model_opt.encode(X_va, C_va)[0].cpu().numpy()\n","\n","# ============================================================\n","# 8. LATENT METRICS (INTRA + INTER)\n","# ============================================================\n","\n","rows = []\n","for d in DRIVERS:\n","    Zd = Z_va[drv_va == d]\n","    rows.append({\n","        \"Driver\": d,\n","        \"Cosine\": cosine_similarity(Zd).mean(),\n","        \"Euclidean\": np.mean([euclidean(Zd[i], Zd[j])\n","                              for i in range(len(Zd))\n","                              for j in range(i+1, len(Zd))]),\n","        \"VarTrace\": np.trace(np.cov(Zd.T))\n","    })\n","\n","latent_intra_df = pd.DataFrame(rows)\n","\n","centroids = {d: Z_va[drv_va==d].mean(0) for d in DRIVERS}\n","latent_inter_df = pd.DataFrame([\n","    {\n","        \"Pair\": f\"{d1}-{d2}\",\n","        \"Centroid_Euclid\": euclidean(centroids[d1], centroids[d2])\n","    }\n","    for i,d1 in enumerate(DRIVERS) for d2 in DRIVERS[i+1:]\n","])\n","\n","sil = silhouette_score(Z_va, le_drv.transform(drv_va))\n","db  = davies_bouldin_score(Z_va, le_drv.transform(drv_va))\n","\n","# ============================================================\n","# 9. PCA + t-SNE\n","# ============================================================\n","\n","Z_pca = PCA(2).fit_transform(Z_va)\n","Z_tsne = TSNE(2, perplexity=30, random_state=SEED).fit_transform(Z_va)\n","\n","plt.figure(figsize=(6,5))\n","sns.scatterplot(x=Z_pca[:,0], y=Z_pca[:,1], hue=drv_va)\n","plt.title(\"SIGNet-v3 PCA\")\n","plt.savefig(f\"{DIRS['fig']}/latent_pca.png\", dpi=300)\n","plt.close()\n","\n","plt.figure(figsize=(6,5))\n","sns.scatterplot(x=Z_tsne[:,0], y=Z_tsne[:,1], hue=drv_va)\n","plt.title(\"SIGNet-v3 t-SNE\")\n","plt.savefig(f\"{DIRS['fig']}/latent_tsne.png\", dpi=300)\n","plt.close()\n","\n","# ============================================================\n","# 10. IDINFERNET-v3\n","# ============================================================\n","\n","class IDInferNetV3(nn.Module):\n","    def __init__(self, z_dim, n):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(z_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, n)\n","        )\n","    def forward(self, z): return self.net(z)\n","\n","y = le_drv.transform(drv_va)\n","Ztr, Zte, ytr, yte = train_test_split(\n","    Z_va, y, stratify=y, test_size=0.3, random_state=SEED\n",")\n","\n","Ztr = torch.tensor(Ztr, dtype=torch.float32).to(device)\n","Zte = torch.tensor(Zte, dtype=torch.float32).to(device)\n","ytr = torch.tensor(ytr, dtype=torch.long).to(device)\n","yte = torch.tensor(yte, dtype=torch.long).to(device)\n","\n","idnet = IDInferNetV3(Ztr.shape[1], len(DRIVERS)).to(device)\n","opt = optim.Adam(idnet.parameters(), lr=1e-3)\n","\n","acc_hist = []\n","for _ in range(200):\n","    opt.zero_grad()\n","    loss = F.cross_entropy(idnet(Ztr), ytr)\n","    loss.backward()\n","    opt.step()\n","    acc_hist.append(\n","        accuracy_score(yte.cpu(), idnet(Zte).argmax(1).cpu())\n","    )\n","\n","# ============================================================\n","# 11. DT-GDIN-v3\n","# ============================================================\n","\n","signatures = {d: centroids[d] for d in DRIVERS}\n","Xs, ys = [], []\n","\n","for d in DRIVERS:\n","    mu = torch.tensor(signatures[d]).to(device)\n","    Cd = C[df_all[\"Driver\"] == d]\n","    for _ in range(3000):\n","        z = mu + 0.6 * torch.randn_like(mu)\n","        c = Cd[np.random.randint(len(Cd))]\n","        with torch.no_grad():\n","            xg = model_opt.decode(z.unsqueeze(0), c.unsqueeze(0))\n","        Xs.append(xg.cpu().numpy()[0])\n","        ys.append(d)\n","\n","Xs = np.array(Xs)\n","ys = le_drv.transform(ys)\n","\n","Xtr, Xva, ytr, yva = train_test_split(\n","    Xs, ys, stratify=ys, test_size=0.3, random_state=SEED\n",")\n","\n","dtgdin = IDInferNetV3(Xtr.shape[1], len(DRIVERS)).to(device)\n","opt = optim.Adam(dtgdin.parameters(), lr=1e-3)\n","\n","dt_acc = []\n","for _ in range(200):\n","    opt.zero_grad()\n","    loss = F.cross_entropy(dtgdin(\n","        torch.tensor(Xtr, dtype=torch.float32).to(device)),\n","        torch.tensor(ytr, dtype=torch.long).to(device))\n","    loss.backward()\n","    opt.step()\n","    dt_acc.append(\n","        accuracy_score(yva,\n","            dtgdin(torch.tensor(Xva, dtype=torch.float32).to(device))\n","            .argmax(1).cpu())\n","    )\n","\n","\n","# ============================================================\n","# 13. IDINFERNET-v3 FULL EVALUATION\n","# ============================================================\n","\n","start = time.time()\n","with torch.no_grad():\n","    y_pred = idnet(Zte).argmax(1).cpu().numpy()\n","    y_prob = F.softmax(idnet(Zte), dim=1).cpu().numpy()\n","idinfer_infer_time = (time.time() - start) / len(Zte) * 1000\n","\n","cm_id = confusion_matrix(yte.cpu(), y_pred)\n","\n","plt.figure(figsize=(5,4))\n","sns.heatmap(cm_id, annot=True, fmt=\"d\",\n","            xticklabels=le_drv.classes_,\n","            yticklabels=le_drv.classes_)\n","plt.title(\"IDInferNet-v3 Confusion Matrix\")\n","plt.tight_layout()\n","plt.savefig(f\"{DIRS['fig']}/idinfernet_v3_cm.png\", dpi=300)\n","plt.close()\n","\n","id_report_df = pd.DataFrame(\n","    classification_report(\n","        yte.cpu(), y_pred,\n","        target_names=le_drv.classes_,\n","        output_dict=True\n","    )\n",").T\n","\n","from sklearn.preprocessing import label_binarize\n","\n","y_bin = label_binarize(yte.cpu(), classes=range(len(DRIVERS)))\n","roc_rows = []\n","\n","plt.figure(figsize=(6,5))\n","for i, drv in enumerate(le_drv.classes_):\n","    fpr, tpr, _ = roc_curve(y_bin[:, i], y_prob[:, i])\n","    auc_i = auc(fpr, tpr)\n","    roc_rows.append({\"Driver\": drv, \"AUC\": auc_i})\n","    plt.plot(fpr, tpr, label=f\"{drv} (AUC={auc_i:.3f})\")\n","\n","plt.plot([0,1],[0,1],'k--')\n","plt.legend()\n","plt.title(\"IDInferNet-v3 ROC\")\n","plt.tight_layout()\n","plt.savefig(f\"{DIRS['fig']}/idinfernet_v3_roc.png\", dpi=300)\n","plt.close()\n","\n","id_roc_df = pd.DataFrame(roc_rows)\n","start = time.time()\n","with torch.no_grad():\n","    dt_pred = dtgdin(torch.tensor(Xva, dtype=torch.float32).to(device)) \\\n","        .argmax(1).cpu().numpy()\n","dtgdin_infer_time = (time.time() - start) / len(Xva) * 1000\n","\n","cm_dt = confusion_matrix(yva, dt_pred)\n","\n","plt.figure(figsize=(5,4))\n","sns.heatmap(cm_dt, annot=True, fmt=\"d\",\n","            xticklabels=le_drv.classes_,\n","            yticklabels=le_drv.classes_)\n","plt.title(\"DT-GDIN-v3 Confusion Matrix\")\n","plt.tight_layout()\n","plt.savefig(f\"{DIRS['fig']}/dtgdin_v3_cm.png\", dpi=300)\n","plt.close()\n","\n","dt_report_df = pd.DataFrame(\n","    classification_report(\n","        yva, dt_pred,\n","        target_names=le_drv.classes_,\n","        output_dict=True\n","    )\n",").T\n","# ============================================================\n","# 16. EDGE METRICS\n","# ============================================================\n","\n","with torch.no_grad():\n","    flops, params = profile(\n","        model_opt,\n","        inputs=(X_va[:1], C_va[:1]),\n","        verbose=False\n","    )\n","\n","edge_df = pd.DataFrame([{\n","    \"SIGNet_FLOPs_M\": flops / 1e6,\n","    \"SIGNet_Params_M\": params / 1e6,\n","    \"IDInferNet_Infer_ms\": idinfer_infer_time,\n","    \"DTGDIN_Infer_ms\": dtgdin_infer_time\n","}])\n","plt.figure()\n","plt.plot(acc_hist)\n","plt.title(\"IDInferNet-v3 Accuracy\")\n","plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\n","plt.savefig(f\"{DIRS['fig']}/idinfernet_v3_acc.png\", dpi=300)\n","plt.close()\n","\n","plt.figure()\n","plt.plot(dt_acc)\n","plt.title(\"DT-GDIN-v3 Accuracy\")\n","plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\n","plt.savefig(f\"{DIRS['fig']}/dtgdin_v3_acc.png\", dpi=300)\n","plt.close()\n","\n","# ============================================================\n","# 12. SAVE MASTER EXCEL\n","# ============================================================\n","\n","excel_path = f\"{DIRS['xls']}/SIGNetV3_MasterResults.xlsx\"\n","with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as w:\n","\n","    pd.DataFrame(hist_base).to_excel(w,\"SIGNet_Base\",index=False)\n","    pd.DataFrame(hist_opt).to_excel(w,\"SIGNet_Optimized\",index=False)\n","\n","    latent_intra_df.to_excel(w,\"Latent_Intra\",index=False)\n","    latent_inter_df.to_excel(w,\"Latent_Inter\",index=False)\n","\n","    pd.DataFrame({\n","        \"Silhouette\":[sil],\n","        \"Davies_Bouldin\":[db]\n","    }).to_excel(w,\"Cluster_Quality\",index=False)\n","\n","    pd.DataFrame(acc_hist, columns=[\"Accuracy\"]).to_excel(w,\"IDInferNet_Ablation\",index=False)\n","    id_report_df.to_excel(w,\"IDInferNet_Report\")\n","    pd.DataFrame(cm_id).to_excel(w,\"IDInferNet_Confusion\")\n","    id_roc_df.to_excel(w,\"IDInferNet_ROC\")\n","\n","    pd.DataFrame(dt_acc, columns=[\"Accuracy\"]).to_excel(w,\"DT_GDIN_Ablation\",index=False)\n","    dt_report_df.to_excel(w,\"DT_GDIN_Report\")\n","    pd.DataFrame(cm_dt).to_excel(w,\"DT_GDIN_Confusion\")\n","\n","    edge_df.to_excel(w,\"Edge_Metrics\",index=False)\n","\n","\n","print(\"‚úÖ SIGNet-v3 COMPLETE\")\n","print(f\"üìÅ Saved to: {RES_ROOT}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2JYN31tmjdB"},"outputs":[],"source":[]},{"cell_type":"code","source":["# ============================================================\n","# CVAE_Digital_Twin-v3 | MASTER TRAIN + OPTIMIZATION\n","# SIGNet-v3 + IDInferNet-v3 + DT-GDIN-v3\n","# FP32 + Pruned + Quantized\n","# ============================================================\n","\n","!pip -q install thop openpyxl\n","\n","# ============================================================\n","# 0. IMPORTS & GLOBALS\n","# ============================================================\n","\n","import os, time, copy\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.utils.prune as prune\n","\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from thop import profile\n","from google.colab import drive\n","\n","SEED = 42\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","# ============================================================\n","# 1. PATHS\n","# ============================================================\n","\n","drive.mount(\"/content/drive\")\n","\n","DATA_ROOT = \"/content/drive/MyDrive/DT_Driver_Wise_Data\"\n","RES_ROOT  = f\"{DATA_ROOT}/SIGNetV3_Results\"\n","\n","DIRS = {\n","    \"mdl\": f\"{RES_ROOT}/Models\",\n","    \"xls\": f\"{RES_ROOT}/Excel\",\n","}\n","for d in DIRS.values():\n","    os.makedirs(d, exist_ok=True)\n","\n","# ============================================================\n","# 2. DATA LOADING (REAL CSVs)\n","# ============================================================\n","\n","DRIVERS = [\"B\", \"D\", \"F\"]\n","FEATURES = [\n","    \"Long_Term_Fuel_Trim_Bank1\",\n","    \"Engine_coolant_temperature.1\",\n","    \"Activation_of_Air_compressor\",\n","    \"Torque_of_friction\",\n","    \"Engine_soacking_time\",\n","    \"Intake_air_pressure\",\n","]\n","TIME_COL = \"Time(s)\"\n","\n","dfs = []\n","for d in DRIVERS:\n","    for s in [\"Train\", \"Valid\"]:\n","        p = f\"{DATA_ROOT}/Driver_{d}_{s}.csv\"\n","        if not os.path.exists(p):\n","            raise FileNotFoundError(p)\n","        df = pd.read_csv(p)\n","        df[\"Driver\"] = d\n","        df[\"Split\"] = s\n","        dfs.append(df)\n","\n","df_all = pd.concat(dfs, ignore_index=True)\n","df_all = df_all.dropna(subset=FEATURES + [TIME_COL])\n","\n","# ============================================================\n","# 3. PREPROCESSING (UNCHANGED)\n","# ============================================================\n","\n","sc_x, sc_t = StandardScaler(), StandardScaler()\n","enc = OneHotEncoder(sparse_output=False)\n","le  = LabelEncoder()\n","\n","X = sc_x.fit_transform(df_all[FEATURES])\n","T = sc_t.fit_transform(df_all[[TIME_COL]])\n","D = enc.fit_transform(df_all[[\"Driver\"]])\n","C = np.concatenate([T, D], axis=1)\n","\n","X = torch.tensor(X, dtype=torch.float32)\n","C = torch.tensor(C, dtype=torch.float32)\n","y = torch.tensor(le.fit_transform(df_all[\"Driver\"]), dtype=torch.long)\n","\n","mask_tr = df_all[\"Split\"] == \"Train\"\n","mask_va = df_all[\"Split\"] == \"Valid\"\n","\n","X_tr, C_tr, y_tr = X[mask_tr].to(device), C[mask_tr].to(device), y[mask_tr].to(device)\n","X_va, C_va, y_va = X[mask_va].to(device), C[mask_va].to(device), y[mask_va].to(device)\n","\n","# ============================================================\n","# 4. ARCHITECTURES (EXACT v3)\n","# ============================================================\n","\n","class SIGNetV3(nn.Module):\n","    def __init__(self, x_dim, c_dim, z_dim=32):\n","        super().__init__()\n","        self.enc = nn.Sequential(\n","            nn.Linear(x_dim + c_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 256),\n","            nn.ReLU()\n","        )\n","        self.mu = nn.Linear(256, z_dim)\n","        self.lv = nn.Linear(256, z_dim)\n","        self.dec = nn.Sequential(\n","            nn.Linear(z_dim + c_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, x_dim)\n","        )\n","\n","    def encode(self, x, c):\n","        h = self.enc(torch.cat([x, c], 1))\n","        return self.mu(h), self.lv(h)\n","\n","    def reparam(self, mu, lv):\n","        return mu + torch.randn_like(mu) * torch.exp(0.5 * lv)\n","\n","    def forward(self, x, c):\n","        mu, lv = self.encode(x, c)\n","        z = self.reparam(mu, lv)\n","        return self.dec(torch.cat([z, c], 1)), mu, lv\n","\n","\n","class IDInferNetV3(nn.Module):\n","    def __init__(self, z, n):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(z, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, n)\n","        )\n","    def forward(self, z): return self.net(z)\n","\n","\n","class DTGDINV3(nn.Module):\n","    def __init__(self, d, n):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(d, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, n)\n","        )\n","    def forward(self, x): return self.net(x)\n","\n","# ============================================================\n","# 5. TRAIN (AUTO IF WEIGHTS MISSING)\n","# ============================================================\n","\n","signet_p = f\"{DIRS['mdl']}/signet_v3.pt\"\n","idnet_p  = f\"{DIRS['mdl']}/idinfernet_v3.pt\"\n","dtgdin_p = f\"{DIRS['mdl']}/dtgdin_v3.pt\"\n","\n","if not (os.path.exists(signet_p) and os.path.exists(idnet_p) and os.path.exists(dtgdin_p)):\n","    print(\"Training v3 models...\")\n","\n","    signet = SIGNetV3(X.shape[1], C.shape[1]).to(device)\n","    opt = torch.optim.Adam(signet.parameters(), 1e-3)\n","\n","    for _ in range(250):\n","        opt.zero_grad()\n","        xr, mu, lv = signet(X_tr, C_tr)\n","        loss = F.mse_loss(xr, X_tr) - 0.5 * torch.mean(1 + lv - mu**2 - lv.exp())\n","        loss.backward()\n","        opt.step()\n","\n","    torch.save(signet.state_dict(), signet_p)\n","\n","    with torch.no_grad():\n","        Z_tr = signet.encode(X_tr, C_tr)[0]\n","        Z_va = signet.encode(X_va, C_va)[0]\n","\n","    idnet = IDInferNetV3(Z_tr.shape[1], len(DRIVERS)).to(device)\n","    opt = torch.optim.Adam(idnet.parameters(), 1e-3)\n","    for _ in range(200):\n","        opt.zero_grad()\n","        F.cross_entropy(idnet(Z_tr), y_tr).backward()\n","        opt.step()\n","    torch.save(idnet.state_dict(), idnet_p)\n","\n","    dtgdin = DTGDINV3(X_tr.shape[1], len(DRIVERS)).to(device)\n","    opt = torch.optim.Adam(dtgdin.parameters(), 1e-3)\n","    for _ in range(200):\n","        opt.zero_grad()\n","        F.cross_entropy(dtgdin(X_tr), y_tr).backward()\n","        opt.step()\n","    torch.save(dtgdin.state_dict(), dtgdin_p)\n","\n","# ============================================================\n","# 6. LOAD MODELS\n","# ============================================================\n","\n","signet = SIGNetV3(X.shape[1], C.shape[1]).to(device)\n","signet.load_state_dict(torch.load(signet_p))\n","signet.eval()\n","\n","with torch.no_grad():\n","    Z_va = signet.encode(X_va, C_va)[0]\n","\n","idnet = IDInferNetV3(Z_va.shape[1], len(DRIVERS)).to(device)\n","idnet.load_state_dict(torch.load(idnet_p))\n","idnet.eval()\n","\n","dtgdin = DTGDINV3(X_va.shape[1], len(DRIVERS)).to(device)\n","dtgdin.load_state_dict(torch.load(dtgdin_p))\n","dtgdin.eval()\n","\n","# ============================================================\n","# 7. OPTIMIZATION (FP32 / PRUNED / INT8)\n","# ============================================================\n","\n","def edge_profile(model, inputs, dtype_bytes):\n","    try:\n","        dev = next(model.parameters()).device\n","    except StopIteration:\n","        dev = torch.device(\"cpu\")\n","\n","    inputs = tuple(i.to(dev) for i in inputs)\n","    with torch.no_grad():\n","        flops, params = profile(model, inputs=inputs, verbose=False)\n","        t0 = time.time()\n","        for _ in range(50):\n","            _ = model(*inputs)\n","        latency = (time.time() - t0)/50*1000\n","\n","    return {\n","        \"Params\": params,\n","        \"FLOPs\": flops,\n","        \"Latency_ms\": latency,\n","        \"Memory_KB\": params * dtype_bytes / 1024\n","    }\n","\n","def prune_model(m):\n","    m = copy.deepcopy(m)\n","    for l in m.modules():\n","        if isinstance(l, nn.Linear):\n","            prune.ln_structured(l, \"weight\", 0.3, n=1, dim=0)\n","            prune.remove(l, \"weight\")\n","    return m\n","\n","def quant_model(m):\n","    return torch.quantization.quantize_dynamic(m.cpu(), {nn.Linear}, torch.qint8)\n","\n","models = {\n","    \"SIGNetV3_FP32\": signet,\n","    \"SIGNetV3_Pruned\": prune_model(signet),\n","    \"SIGNetV3_Quant\": quant_model(signet),\n","}\n","\n","rows = []\n","for n, m in models.items():\n","    rows.append({\n","        \"Model\": n,\n","        **edge_profile(m, (X_va[:1], C_va[:1]), 1 if \"Quant\" in n else 4)\n","    })\n","    torch.save(m.state_dict(), f\"{DIRS['mdl']}/{n}.pt\")\n","\n","pd.DataFrame(rows).to_excel(f\"{DIRS['xls']}/SIGNetV3_Edge.xlsx\", index=False)\n","\n","print(\"‚úÖ SIGNet-v3 TRAIN + OPTIMIZATION COMPLETE\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YpP5X5BtWdjL","executionInfo":{"status":"ok","timestamp":1765992531007,"user_tz":-300,"elapsed":65245,"user":{"displayName":"Community Of Research & Development","userId":"07614819381775789507"}},"outputId":"94f3a5f6-0723-4ec3-8d3e-72c99fa1eaef"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","Mounted at /content/drive\n","Training v3 models...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-303325861.py:253: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n","For migrations of users: \n","1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n","2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n","3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n","see https://github.com/pytorch/ao/issues/2259 for more details\n","  return torch.quantization.quantize_dynamic(m.cpu(), {nn.Linear}, torch.qint8)\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ SIGNet-v3 TRAIN + OPTIMIZATION COMPLETE\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"A1iuvFNNWkLJ"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1Jmc1ngD3HeOl8iMhjO5aLujA4iMpyLKY","timestamp":1765623550439}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}