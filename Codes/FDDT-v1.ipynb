{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Jmc1ngD3HeOl8iMhjO5aLujA4iMpyLKY","timestamp":1765623049788}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<div style=\"font-family:Arial;\">\n","\n","<h2 style=\"color:#1f4fd8;\">SIGNet ‚Äî Driver Digital Twin Generator (CVAE)</h2>\n","\n","<div style=\"display:flex; align-items:center; justify-content:center; gap:20px;\">\n","\n","  <!-- Input -->\n","  <div style=\"background:#e3f2fd; padding:14px; border-radius:8px; text-align:center; width:170px;\">\n","    <b>Vehicle Features</b><br/>\n","    Engine ¬∑ Torque ¬∑ Intake<br/>\n","    <hr/>\n","    <b>Context</b><br/>\n","    Time + Driver ID\n","  </div>\n","\n","  <!-- Arrow -->\n","  <div style=\"font-size:28px;\">‚û°Ô∏è</div>\n","\n","  <!-- Encoder -->\n","  <div style=\"background:#bbdefb; padding:14px; border-radius:8px; text-align:center; width:180px;\">\n","    <b>Encoder</b><br/>\n","    Fully Connected Layers<br/>\n","    ReLU\n","  </div>\n","\n","  <div style=\"font-size:28px;\">‚û°Ô∏è</div>\n","\n","  <!-- Latent -->\n","  <div style=\"background:#ede7f6; padding:14px; border-radius:8px; text-align:center; width:200px;\">\n","    <b>Latent Space (z)</b><br/>\n","    Œº (mean), œÉ¬≤ (variance)<br/>\n","    <i>Driver Digital Twin</i>\n","  </div>\n","\n","  <div style=\"font-size:28px;\">‚û°Ô∏è</div>\n","\n","  <!-- Decoder -->\n","  <div style=\"background:#c8e6c9; padding:14px; border-radius:8px; text-align:center; width:180px;\">\n","    <b>Decoder</b><br/>\n","    z + Context<br/>\n","    Fully Connected Layers\n","  </div>\n","\n","  <div style=\"font-size:28px;\">‚û°Ô∏è</div>\n","\n","  <!-- Output -->\n","  <div style=\"background:#e8f5e9; padding:14px; border-radius:8px; text-align:center; width:170px;\">\n","    <b>Reconstructed / Generated</b><br/>\n","    Vehicle Signals\n","  </div>\n","\n","</div>\n","\n","<p style=\"margin-top:12px;\">\n","<b>Purpose:</b> Learn a compact, stable, and separable latent representation that acts as a\n","<b>driver-specific digital twin</b>.\n","</p>\n","\n","</div>\n"],"metadata":{"id":"QdR5z1IhV1B8"}},{"cell_type":"markdown","source":["![download.svg](data:image/svg+xml;base64,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)"],"metadata":{"id":"W-Lf9TTIYu8W"}},{"cell_type":"markdown","source":["<div style=\"font-family:Arial;\">\n","\n","<h2 style=\"color:#1f4fd8;\">IDInferNet ‚Äî Real Latent Driver Identification</h2>\n","\n","<div style=\"display:flex; align-items:center; justify-content:center; gap:20px;\">\n","\n","  <!-- Latent Input -->\n","  <div style=\"background:#ede7f6; padding:14px; border-radius:8px; text-align:center; width:200px;\">\n","    <b>Latent Vectors (z)</b><br/>\n","    From SIGNet Encoder<br/>\n","    <i>Real Data</i>\n","  </div>\n","\n","  <div style=\"font-size:28px;\">‚û°Ô∏è</div>\n","\n","  <!-- MLP -->\n","  <div style=\"background:#fff3e0; padding:14px; border-radius:8px; text-align:center; width:200px;\">\n","    <b>MLP Classifier</b><br/>\n","    FC ‚Üí ReLU ‚Üí FC\n","  </div>\n","\n","  <div style=\"font-size:28px;\">‚û°Ô∏è</div>\n","\n","  <!-- Output -->\n","  <div style=\"background:#ffe0b2; padding:14px; border-radius:8px; text-align:center; width:170px;\">\n","    <b>Driver Prediction</b><br/>\n","    B ¬∑ D ¬∑ F\n","  </div>\n","\n","</div>\n","\n","<p style=\"margin-top:12px;\">\n","<b>Purpose:</b> Validate that <b>real latent embeddings</b> learned by SIGNet preserve\n","driver identity and are discriminative.\n","</p>\n","\n","</div>\n"],"metadata":{"id":"jZEeEe-JV7st"}},{"cell_type":"markdown","source":["![download (1).svg](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iOTAwIiBoZWlnaHQ9IjMwMCIgdmlld0JveD0iMCAwIDkwMCAzMDAiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgc3R5bGU9ImZvbnQtZmFtaWx5OkFyaWFsLCBzYW5zLXNlcmlmOyI+CgogIDx0ZXh0IHg9IjIwIiB5PSIzMCIgZm9udC1zaXplPSIxOCIgZm9udC13ZWlnaHQ9ImJvbGQiPgogICAgSURJbmZlck5ldCDigJQgUmVhbCBMYXRlbnQgRHJpdmVyIElkZW50aWZpY2F0aW9uCiAgPC90ZXh0PgoKICA8IS0tIEJsb2NrcyAtLT4KICA8cmVjdCB4PSI4MCIgeT0iMTIwIiB3aWR0aD0iMjIwIiBoZWlnaHQ9IjYwIiBmaWxsPSIjZDFjNGU5IiBzdHJva2U9IiMzMzMiPjwvcmVjdD4KICA8dGV4dCB4PSIxOTAiIHk9IjE1NSIgdGV4dC1hbmNob3I9Im1pZGRsZSI+TGF0ZW50IFZlY3RvcnMgejwvdGV4dD4KCiAgPHJlY3QgeD0iMzYwIiB5PSIxMjAiIHdpZHRoPSIyMjAiIGhlaWdodD0iNjAiIGZpbGw9IiNmZmUwYjIiIHN0cm9rZT0iIzMzMyI+PC9yZWN0PgogIDx0ZXh0IHg9IjQ3MCIgeT0iMTU1IiB0ZXh0LWFuY2hvcj0ibWlkZGxlIj5NTFAgQ2xhc3NpZmllcjwvdGV4dD4KCiAgPHJlY3QgeD0iNjQwIiB5PSIxMjAiIHdpZHRoPSIyMjAiIGhlaWdodD0iNjAiIGZpbGw9IiNmZmNjODAiIHN0cm9rZT0iIzMzMyI+PC9yZWN0PgogIDx0ZXh0IHg9Ijc1MCIgeT0iMTU1IiB0ZXh0LWFuY2hvcj0ibWlkZGxlIj5Ecml2ZXIgUHJlZGljdGlvbjwvdGV4dD4KCiAgPCEtLSBMb3NzIC0tPgogIDxyZWN0IHg9IjM2MCIgeT0iMjEwIiB3aWR0aD0iMjIwIiBoZWlnaHQ9IjYwIiBmaWxsPSIjZmZjZGQyIiBzdHJva2U9IiMzMzMiPjwvcmVjdD4KICA8dGV4dCB4PSI0NzAiIHk9IjI0NSIgdGV4dC1hbmNob3I9Im1pZGRsZSI+Q3Jvc3MtRW50cm9weSBMb3NzPC90ZXh0PgoKICA8IS0tIEFycm93cyAtLT4KICA8ZGVmcz4KICAgIDxtYXJrZXIgaWQ9ImFycm93MiIgbWFya2VyV2lkdGg9IjEwIiBtYXJrZXJIZWlnaHQ9IjEwIiByZWZYPSI5IiByZWZZPSIzIiBvcmllbnQ9ImF1dG8iPgogICAgICA8cGF0aCBkPSJNMCwwIEwwLDYgTDksMyB6IiBmaWxsPSIjMzMzIj48L3BhdGg+CiAgICA8L21hcmtlcj4KICA8L2RlZnM+CgogIDxsaW5lIHgxPSIzMDAiIHkxPSIxNTAiIHgyPSIzNjAiIHkyPSIxNTAiIHN0cm9rZT0iIzMzMyIgbWFya2VyLWVuZD0idXJsKCNhcnJvdzIpIj48L2xpbmU+CiAgPGxpbmUgeDE9IjU4MCIgeTE9IjE1MCIgeDI9IjY0MCIgeTI9IjE1MCIgc3Ryb2tlPSIjMzMzIiBtYXJrZXItZW5kPSJ1cmwoI2Fycm93MikiPjwvbGluZT4KICA8bGluZSB4MT0iNDcwIiB5MT0iMjEwIiB4Mj0iNDcwIiB5Mj0iMTgwIiBzdHJva2U9IiNjNjI4MjgiIG1hcmtlci1lbmQ9InVybCgjYXJyb3cyKSI+PC9saW5lPgoKPC9zdmc+Cg==)"],"metadata":{"id":"6tOG6uz2aAa_"}},{"cell_type":"markdown","source":["<div style=\"font-family:Arial;\">\n","\n","<h2 style=\"color:#1f4fd8;\">DT-GDIN ‚Äî Digital-Twin Generated Driver Identification Network</h2>\n","\n","<div style=\"display:flex; align-items:center; justify-content:center; gap:18px;\">\n","\n","  <!-- Signature -->\n","  <div style=\"background:#d1c4e9; padding:14px; border-radius:8px; text-align:center; width:190px;\">\n","    <b>Digital Twin Signature</b><br/>\n","    Mean Latent Vector<br/>\n","    per Driver\n","  </div>\n","\n","  <div style=\"font-size:26px;\">‚ûï</div>\n","\n","  <!-- Noise -->\n","  <div style=\"background:#f3e5f5; padding:14px; border-radius:8px; text-align:center; width:140px;\">\n","    <b>Latent Noise</b><br/>\n","    Œµ ~ N(0, œÉ)\n","  </div>\n","\n","  <div style=\"font-size:26px;\">‚û°Ô∏è</div>\n","\n","  <!-- Decoder -->\n","  <div style=\"background:#c8e6c9; padding:14px; border-radius:8px; text-align:center; width:190px;\">\n","    <b>SIGNet Decoder</b><br/>\n","    Generates Virtual Signals\n","  </div>\n","\n","  <div style=\"font-size:26px;\">‚û°Ô∏è</div>\n","\n","  <!-- Classifier -->\n","  <div style=\"background:#fff3e0; padding:14px; border-radius:8px; text-align:center; width:190px;\">\n","    <b>DT-GDIN Classifier</b><br/>\n","    FC ‚Üí ReLU ‚Üí FC\n","  </div>\n","\n","  <div style=\"font-size:26px;\">‚û°Ô∏è</div>\n","\n","  <!-- Output -->\n","  <div style=\"background:#ffe0b2; padding:14px; border-radius:8px; text-align:center; width:160px;\">\n","    <b>Driver Prediction</b><br/>\n","    (Generated Data)\n","  </div>\n","\n","</div>\n","\n","<p style=\"margin-top:12px;\">\n","<b>Purpose:</b> Verify that the <b>digital twin generalizes</b> beyond real data.\n","High performance implies that generated behavior preserves driver identity.\n","</p>\n","\n","</div>\n"],"metadata":{"id":"zCddHWxFWCNQ"}},{"cell_type":"markdown","source":["![download (2).svg](data:image/svg+xml;base64,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)"],"metadata":{"id":"XbmzibDAaDIi"}},{"cell_type":"markdown","source":["<!-- =========================================================\n"," CVAE_Digital_Twin-v1\n"," Digital Twin‚ÄìDriven Driver Modeling & Identification\n"," IEEE Transactions‚ÄìGrade Methodology Overview\n","========================================================= -->\n","\n","<div style=\"\n","    font-family: 'Segoe UI', Roboto, Arial, sans-serif;\n","    line-height: 1.65;\n","    color:#222;\n","\">\n","\n","<h1 style=\"color:#1f4fd8; margin-bottom:4px;\">\n","üöó Digital Twin‚ÄìDriven Driver Modeling with SIGNet\n","</h1>\n","\n","<p style=\"font-size:16px; margin-top:0;\">\n","<b>SIGNet + IDInferNet + DT-GDIN</b><br/>\n","A complete, reproducible framework for <b>learning, validating, and exploiting\n","driver-specific digital twins</b>.\n","</p>\n","\n","<hr/>\n","\n","<h2 style=\"color:#0b6fa4;\">1. Problem Context & Motivation</h2>\n","\n","<p>\n","Modern intelligent vehicles require <b>driver-aware models</b> for personalization,\n","safety, and adaptive control. However, real-world driving datasets are:\n","</p>\n","\n","<ul>\n","  <li>Limited in coverage of rare or extreme behaviors</li>\n","  <li>Highly heterogeneous across drivers</li>\n","  <li>Expensive to collect at scale</li>\n","</ul>\n","\n","<p>\n","This motivates a <b>Digital Twin paradigm</b>, where each driver is represented by\n","a compact generative model capable of:\n","</p>\n","\n","<ul>\n","  <li>Encoding behavioral characteristics</li>\n","  <li>Generating realistic virtual samples</li>\n","  <li>Supporting downstream identification and analysis</li>\n","</ul>\n","\n","<hr/>\n","\n","<h2 style=\"color:#0b6fa4;\">2. Core Contribution & Novelty</h2>\n","\n","<div style=\"\n","    background:#eef6ff;\n","    padding:14px;\n","    border-left:5px solid #1f4fd8;\n","\">\n","<b>Key idea:</b><br/>\n","The latent space learned by SIGNet is <u>explicitly treated as a driver digital twin</u>,\n","not merely as a reconstruction artifact.\n","</div>\n","\n","<p>\n","Unlike conventional CVAE pipelines, this work introduces a\n","<b>three-tier validation strategy</b>:\n","</p>\n","\n","<ol>\n","  <li><b>SIGNet</b> learns conditional latent representations of driver behavior</li>\n","  <li><b>IDInferNet</b> validates discriminability on <i>real latent embeddings</i></li>\n","  <li><b>DT-GDIN</b> validates identity preservation on <i>generated digital-twin samples</i></li>\n","</ol>\n","\n","<p>\n","This ensures that the learned digital twin is:\n","</p>\n","\n","<ul>\n","  <li>Compact</li>\n","  <li>Stable</li>\n","  <li>Separable</li>\n","  <li>Generative beyond observed data</li>\n","</ul>\n","\n","<hr/>\n","\n","<h2 style=\"color:#0b6fa4;\">3. SIGNet ‚Äî Digital Twin Generator</h2>\n","\n","<p>\n","SIGNet is a <b>Conditional Variational Autoencoder (CVAE)</b> trained on:\n","</p>\n","\n","<ul>\n","  <li>Vehicle sensor features (engine, torque, intake, etc.)</li>\n","  <li>Context variables (time + driver identity)</li>\n","</ul>\n","\n","<p>\n","The encoder maps inputs into a low-dimensional latent vector <code>z</code>,\n","which is interpreted as the <b>instantaneous digital twin state</b>.\n","</p>\n","\n","<p>\n","Two training regimes are evaluated:\n","</p>\n","\n","<ul>\n","  <li><b>Baseline</b> ‚Äî standard CVAE training</li>\n","  <li><b>Optimized</b> ‚Äî tuned learning rate + early stopping</li>\n","</ul>\n","\n","<p>\n","Training and validation losses (total, reconstruction, KL) are logged,\n","plotted, and saved for ablation analysis.\n","</p>\n","\n","<hr/>\n","\n","<h2 style=\"color:#0b6fa4;\">4. Digital Twin Signatures</h2>\n","\n","<p>\n","For each driver, a <b>digital twin signature</b> is computed as:\n","</p>\n","\n","<div style=\"background:#f7f7f7; padding:10px;\">\n","Mean latent vector across all samples belonging to that driver\n","</div>\n","\n","<p>\n","These signatures serve three roles:\n","</p>\n","\n","<ul>\n","  <li>Compact driver descriptors</li>\n","  <li>Anchors for latent consistency analysis</li>\n","  <li>Seeds for digital-twin data generation</li>\n","</ul>\n","\n","<hr/>\n","\n","<h2 style=\"color:#0b6fa4;\">5. Latent Consistency & Separability Analysis</h2>\n","\n","<p>\n","A comprehensive <b>IEEE Transactions‚Äìaligned metric suite</b>\n","is used to validate the digital twin latent space.\n","</p>\n","\n","<h3 style=\"color:#444;\">Intra-driver consistency</h3>\n","<ul>\n","  <li>Mean cosine similarity</li>\n","  <li>Mean pairwise Euclidean distance</li>\n","  <li>Latent covariance trace</li>\n","  <li>Coefficient of variation</li>\n","</ul>\n","\n","<h3 style=\"color:#444;\">Inter-driver separability</h3>\n","<ul>\n","  <li>Centroid Euclidean distance</li>\n","  <li>Centroid cosine distance</li>\n","  <li>Fisher Discriminant Ratio</li>\n","</ul>\n","\n","<h3 style=\"color:#444;\">Cluster quality</h3>\n","<ul>\n","  <li>Silhouette score</li>\n","  <li>Davies‚ÄìBouldin index</li>\n","</ul>\n","\n","<p>\n","Together, these metrics quantitatively answer:\n","</p>\n","\n","<div style=\"background:#eef6ff; padding:12px;\">\n","‚ÄúHow compact, stable, and separable is each driver‚Äôs digital twin?‚Äù\n","</div>\n","\n","<hr/>\n","\n","<h2 style=\"color:#0b6fa4;\">6. Latent Space Visualization</h2>\n","\n","<p>\n","To complement numerical metrics, the latent space is visualized using:\n","</p>\n","\n","<ul>\n","  <li>PCA (global structure)</li>\n","  <li>t-SNE (local neighborhood structure)</li>\n","</ul>\n","\n","<p>\n","All figures are color-coded by driver identity and saved\n","as publication-quality images.\n","</p>\n","\n","<hr/>\n","\n","<h2 style=\"color:#0b6fa4;\">7. IDInferNet ‚Äî Real Latent Driver Identification</h2>\n","\n","<p>\n","IDInferNet is a lightweight neural classifier trained on\n","<b>real latent embeddings</b> produced by SIGNet.\n","</p>\n","\n","<p>\n","Its purpose is to verify that:\n","</p>\n","\n","<ul>\n","  <li>Driver identity is preserved in the latent space</li>\n","  <li>Latent representations are discriminative</li>\n","</ul>\n","\n","<p>\n","The following artifacts are produced:\n","</p>\n","\n","<ul>\n","  <li>Training loss and validation accuracy curves</li>\n","  <li>Confusion matrix</li>\n","  <li>ROC curves (One-vs-Rest)</li>\n","  <li>Classification report</li>\n","  <li>Training time and inference latency</li>\n","</ul>\n","\n","<hr/>\n","\n","<h2 style=\"color:#0b6fa4;\">8. DT-GDIN ‚Äî Digital-Twin Generated Driver Identification Network</h2>\n","\n","<div style=\"\n","    background:#fff4e6;\n","    padding:14px;\n","    border-left:5px solid #e67e22;\n","\">\n","<b>Most critical validation stage.</b><br/>\n","DT-GDIN evaluates the <u>generalization power</u> of the digital twin.\n","</div>\n","\n","<p>\n","Instead of real data, DT-GDIN is trained on:\n","</p>\n","\n","<ul>\n","  <li>Samples generated from digital twin signatures</li>\n","  <li>Decoded through SIGNet under realistic context conditions</li>\n","</ul>\n","\n","<p>\n","High DT-GDIN performance implies that:\n","</p>\n","\n","<ul>\n","  <li>The digital twin encodes identity robustly</li>\n","  <li>Generated behavior remains semantically consistent</li>\n","  <li>The model is suitable for simulation and augmentation</li>\n","</ul>\n","\n","<p>\n","DT-GDIN evaluation includes:\n","</p>\n","\n","<ul>\n","  <li>Accuracy and ablation curves</li>\n","  <li>Confusion matrix</li>\n","  <li>Training time and inference latency</li>\n","</ul>\n","\n","<hr/>\n","\n","<h2 style=\"color:#0b6fa4;\">9. Edge & Deployment Metrics</h2>\n","\n","<p>\n","For practical relevance, all major models are profiled for:\n","</p>\n","\n","<ul>\n","  <li>Training time</li>\n","  <li>Inference latency</li>\n","  <li>FLOPs</li>\n","  <li>Parameter count</li>\n","</ul>\n","\n","<p>\n","These metrics support deployment on\n","<b>resource-constrained automotive edge devices</b>.\n","</p>\n","\n","<hr/>\n"],"metadata":{"id":"mTalVo1_WHJ3"}},{"cell_type":"markdown","source":["![download (4).svg](data:image/svg+xml;base64,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)"],"metadata":{"id":"JXLV918NbneR"}},{"cell_type":"markdown","source":["![download (5).svg](data:image/svg+xml;base64,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)"],"metadata":{"id":"PO3HfzBWbuXl"}},{"cell_type":"markdown","source":["![download (6).svg](data:image/svg+xml;base64,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)"],"metadata":{"id":"hM5qcS_Hb1op"}},{"cell_type":"code","source":["# ============================================================\n","# CVAE_Digital_Twin-v1.ipynb  SIGNet + IDInferNet + DT-GDIN\n","# ============================================================\n","\n","!pip -q install torch torchvision torchaudio thop seaborn scikit-learn openpyxl\n","\n","# ============================================================\n","# SAFE DIRECTORY CREATION (COLAB + DRIVE ROBUST)\n","# ============================================================\n","\n","def ensure_dir(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path, exist_ok=True)\n","\n","# ============================================================\n","# 1. IMPORTS & GLOBAL CONFIG\n","# ============================================================\n","import os, time, copy\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    accuracy_score, classification_report, confusion_matrix\n",")\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.metrics import silhouette_score, davies_bouldin_score\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from scipy.spatial.distance import euclidean\n","from thop import profile\n","\n","SEED = 42\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"‚öôÔ∏è Using device: {device}\")\n","\n","# ============================================================\n","# 2. DATA LOADING & DIRECTORIES\n","# ============================================================\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","DATA_ROOT = \"/content/drive/MyDrive/DT_Driver_Wise_Data\"\n","RES_ROOT  = f\"{DATA_ROOT}/SIGNetV1_Results\"\n","\n","DIRS = {\n","    \"fig\": f\"{RES_ROOT}/Figures\",\n","    \"mdl\": f\"{RES_ROOT}/Models\",\n","    \"xls\": f\"{RES_ROOT}/Excel\"\n","}\n","for d in DIRS.values():\n","    os.makedirs(d, exist_ok=True)\n","\n","DRIVERS = [\"B\", \"D\", \"F\"]\n","FEATURES = [\n","    \"Long_Term_Fuel_Trim_Bank1\",\n","    \"Engine_coolant_temperature.1\",\n","    \"Activation_of_Air_compressor\",\n","    \"Torque_of_friction\",\n","    \"Engine_soacking_time\",\n","    \"Intake_air_pressure\"\n","]\n","TIME_COL = \"Time(s)\"\n","\n","dfs = []\n","for d in DRIVERS:\n","    for s in [\"Train\", \"Valid\"]:\n","        p = f\"{DATA_ROOT}/Driver_{d}_{s}.csv\"\n","        if os.path.exists(p):\n","            df = pd.read_csv(p)\n","            df[\"Driver\"] = d\n","            df[\"Split\"]  = s\n","            dfs.append(df)\n","\n","df_all = pd.concat(dfs, ignore_index=True)\n","df_all = df_all.dropna(subset=FEATURES + [TIME_COL])\n","\n","# ============================================================\n","# GLOBAL TIMING & PROFILING UTILITIES\n","# ============================================================\n","\n","def measure_training_time(train_fn, *args, **kwargs):\n","    start = time.time()\n","    outputs = train_fn(*args, **kwargs)\n","    duration = time.time() - start\n","    return outputs, duration\n","\n","\n","def measure_inference_latency(model, inputs, runs=100):\n","    model.eval()\n","    with torch.no_grad():\n","        start = time.time()\n","        for _ in range(runs):\n","            _ = model(*inputs)\n","    return (time.time() - start) / runs * 1000  # ms\n","\n","# ============================================================\n","# 3. PREPROCESSING\n","# ============================================================\n","sc_x, sc_t = StandardScaler(), StandardScaler()\n","enc = OneHotEncoder(sparse_output=False)\n","\n","X = sc_x.fit_transform(df_all[FEATURES])\n","T = sc_t.fit_transform(df_all[[TIME_COL]])\n","D = enc.fit_transform(df_all[[\"Driver\"]])\n","C = np.concatenate([T, D], axis=1)\n","\n","X = torch.tensor(X, dtype=torch.float32).to(device)\n","C = torch.tensor(C, dtype=torch.float32).to(device)\n","\n","mask_tr = df_all[\"Split\"] == \"Train\"\n","mask_va = df_all[\"Split\"] == \"Valid\"\n","\n","X_tr, C_tr = X[mask_tr], C[mask_tr]\n","X_va, C_va = X[mask_va], C[mask_va]\n","drv_va     = df_all.loc[mask_va, \"Driver\"].values\n","\n","# ============================================================\n","# 4. SIGNet (BASELINE CVAE)\n","# ============================================================\n","class SIGNet(nn.Module):\n","    def __init__(self, x_dim, c_dim, z_dim=8, h=128):\n","        super().__init__()\n","        self.enc = nn.Linear(x_dim + c_dim, h)\n","        self.mu  = nn.Linear(h, z_dim)\n","        self.lv  = nn.Linear(h, z_dim)\n","        self.dec = nn.Linear(z_dim + c_dim, h)\n","        self.out = nn.Linear(h, x_dim)\n","\n","    def encode(self, x, c):\n","        h = F.relu(self.enc(torch.cat([x, c], 1)))\n","        return self.mu(h), self.lv(h)\n","\n","    def reparam(self, mu, lv):\n","        return mu + torch.randn_like(mu) * torch.exp(0.5 * lv)\n","\n","    def decode(self, z, c):\n","        h = F.relu(self.dec(torch.cat([z, c], 1)))\n","        return self.out(h)\n","\n","    def forward(self, x, c):\n","        mu, lv = self.encode(x, c)\n","        z = self.reparam(mu, lv)\n","        return self.decode(z, c), mu, lv\n","\n","def vae_loss(r, x, mu, lv):\n","    rec = F.mse_loss(r, x)\n","    kl  = -0.5 * torch.mean(1 + lv - mu**2 - lv.exp())\n","    return rec + kl, rec, kl\n","\n","# ============================================================\n","# 5. TRAIN BASELINE & OPTIMIZED (EARLY STOPPING)\n","# ============================================================\n","def train_signet(lr):\n","    model = SIGNet(X.shape[1], C.shape[1]).to(device)\n","    opt = optim.Adam(model.parameters(), lr=lr)\n","\n","    hist = {\"total\": [], \"rec\": [], \"kl\": [], \"val\": []}\n","    best, no_imp = np.inf, 0\n","\n","    for ep in range(1, 401):\n","        model.train()\n","        opt.zero_grad()\n","        r, mu, lv = model(X_tr, C_tr)\n","        loss, rec, kl = vae_loss(r, X_tr, mu, lv)\n","        loss.backward()\n","        opt.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            rv, mv, lv_v = model(X_va, C_va)\n","            vloss, _, _ = vae_loss(rv, X_va, mv, lv_v)\n","\n","        hist[\"total\"].append(loss.item())\n","        hist[\"rec\"].append(rec.item())\n","        hist[\"kl\"].append(kl.item())\n","        hist[\"val\"].append(vloss.item())\n","\n","        if vloss < best - 1e-4:\n","            best, best_wts, no_imp = vloss, copy.deepcopy(model.state_dict()), 0\n","        else:\n","            no_imp += 1\n","\n","        if no_imp >= 20:\n","            break\n","\n","    model.load_state_dict(best_wts)\n","    return model, hist\n","\n","model_base, hist_base = train_signet(1e-3)\n","model_opt,  hist_opt  = train_signet(5e-4)\n","\n","torch.save(model_base.state_dict(), f\"{DIRS['mdl']}/signet_v1_baseline.pt\")\n","torch.save(model_opt.state_dict(),  f\"{DIRS['mdl']}/signet_v1_optimized.pt\")\n","\n","# ============================================================\n","# 6. LATENT EXTRACTION (OPTIMIZED MODEL)\n","# ============================================================\n","model_opt.eval()\n","with torch.no_grad():\n","    Z_va = model_opt.encode(X_va, C_va)[0].detach().cpu().numpy()\n","\n","# ============================================================\n","# 7. DIGITAL-TWIN SIGNATURES\n","# ============================================================\n","signatures = {}\n","for d in DRIVERS:\n","    idx = df_all[\"Driver\"] == d\n","    with torch.no_grad():\n","        mu, _ = model_opt.encode(X[idx], C[idx])\n","        signatures[d] = mu.mean(0).detach().cpu().numpy()\n","\n","# ============================================================\n","# 8. LATENT CONSISTENCY METRICS (FULL IEEE SUITE)\n","# ============================================================\n","rows = []\n","\n","for d in DRIVERS:\n","    Zd = Z_va[drv_va == d]\n","\n","    cos_sim = cosine_similarity(Zd).mean()\n","    eucl = np.mean([euclidean(Zd[i], Zd[j])\n","                    for i in range(len(Zd)) for j in range(i+1, len(Zd))])\n","    var_trace = np.trace(np.cov(Zd.T))\n","    cv = np.std(Zd) / (np.mean(np.abs(Zd)) + 1e-8)\n","\n","    rows.append({\n","        \"Driver\": d,\n","        \"Cosine_Similarity\": cos_sim,\n","        \"Mean_Euclidean\": eucl,\n","        \"Variance_Trace\": var_trace,\n","        \"Coeff_Variation\": cv\n","    })\n","\n","latent_intra_df = pd.DataFrame(rows)\n","\n","# Inter-driver\n","pairs = []\n","for i, d1 in enumerate(DRIVERS):\n","    for d2 in DRIVERS[i+1:]:\n","        c1, c2 = signatures[d1], signatures[d2]\n","        pairs.append({\n","            \"Pair\": f\"{d1}-{d2}\",\n","            \"Centroid_Euclidean\": euclidean(c1, c2),\n","            \"Centroid_Cosine\": 1 - cosine_similarity([c1], [c2])[0,0],\n","            \"Fisher_Ratio\":\n","                np.linalg.norm(c1-c2)**2 /\n","                (np.var(Z_va[drv_va==d1]) + np.var(Z_va[drv_va==d2]) + 1e-8)\n","        })\n","\n","latent_inter_df = pd.DataFrame(pairs)\n","\n","sil = silhouette_score(Z_va, LabelEncoder().fit_transform(drv_va))\n","db  = davies_bouldin_score(Z_va, LabelEncoder().fit_transform(drv_va))\n","\n","# ============================================================\n","# 9. LATENT VISUALIZATION\n","# ============================================================\n","pca = PCA(2)\n","Z_pca = pca.fit_transform(Z_va)\n","\n","tsne = TSNE(2, perplexity=30, random_state=SEED)\n","Z_tsne = tsne.fit_transform(Z_va)\n","\n","plt.figure(figsize=(6,5))\n","sns.scatterplot(x=Z_pca[:,0], y=Z_pca[:,1], hue=drv_va)\n","plt.title(\"Latent PCA\"); plt.savefig(f\"{DIRS['fig']}/latent_pca.png\", dpi=300); plt.close()\n","\n","plt.figure(figsize=(6,5))\n","sns.scatterplot(x=Z_tsne[:,0], y=Z_tsne[:,1], hue=drv_va)\n","plt.title(\"Latent t-SNE\"); plt.savefig(f\"{DIRS['fig']}/latent_tsne.png\", dpi=300); plt.close()\n","\n","# ============================================================\n","# 10. DT-GDIN (DIGITAL-TWIN GENERATED CLASSIFIER)\n","# ============================================================\n","Xs, ys = [], []\n","for d in DRIVERS:\n","    mu = torch.tensor(signatures[d]).to(device)\n","    Cd = C[df_all[\"Driver\"] == d]\n","    for _ in range(2000):\n","        z = mu + 0.8 * torch.randn_like(mu)\n","        c = Cd[np.random.randint(len(Cd))]\n","        with torch.no_grad():\n","            x = model_opt.decode(z.unsqueeze(0), c.unsqueeze(0))\n","        Xs.append(x.detach().cpu().numpy()[0])\n","        ys.append(d)\n","\n","Xs, ys = np.array(Xs), np.array(ys)\n","le = LabelEncoder()\n","y_enc = le.fit_transform(ys)\n","\n","Xtr, Xva, ytr, yva = train_test_split(\n","    Xs, y_enc, stratify=y_enc, test_size=0.3, random_state=SEED\n",")\n","\n","class DT_GDIN(nn.Module):\n","    def __init__(self, d, n):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(d, 256), nn.ReLU(),\n","            nn.Linear(256, n)\n","        )\n","    def forward(self, x): return self.net(x)\n","\n","clf = DT_GDIN(Xtr.shape[1], len(DRIVERS)).to(device)\n","opt = optim.Adam(clf.parameters(), lr=1e-3)\n","\n","Xt = torch.tensor(Xtr, dtype=torch.float32).to(device)\n","yt = torch.tensor(ytr, dtype=torch.long).to(device)\n","Xv = torch.tensor(Xva, dtype=torch.float32).to(device)\n","yv = torch.tensor(yva, dtype=torch.long).to(device)\n","\n","acc_hist = []\n","best, no_imp = 0, 0\n","for ep in range(1, 201):\n","    opt.zero_grad()\n","    loss = F.cross_entropy(clf(Xt), yt)\n","    loss.backward()\n","    opt.step()\n","\n","    with torch.no_grad():\n","        acc = accuracy_score(yv.cpu(), clf(Xv).argmax(1).cpu())\n","    acc_hist.append(acc)\n","\n","    if acc > best:\n","        best, best_wts, no_imp = acc, copy.deepcopy(clf.state_dict()), 0\n","    else:\n","        no_imp += 1\n","    if no_imp >= 15:\n","        break\n","\n","clf.load_state_dict(best_wts)\n","torch.save(clf.state_dict(), f\"{DIRS['mdl']}/dt_gdin_v1.pt\")\n","\n","# ============================================================\n","# 10A. IDInferNet ‚Äî REAL LATENT CLASSIFIER (FULL EVALUATION)\n","# ============================================================\n","\n","class IDInferNet(nn.Module):\n","    def __init__(self, z_dim, n_classes):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(z_dim, 128), nn.ReLU(),\n","            nn.Linear(128, n_classes)\n","        )\n","    def forward(self, z): return self.net(z)\n","\n","# Labels\n","le_id = LabelEncoder()\n","y_id = le_id.fit_transform(drv_va)\n","\n","# Train / Test split\n","Ztr, Zte, ytr, yte = train_test_split(\n","    Z_va, y_id, stratify=y_id, test_size=0.3, random_state=SEED\n",")\n","\n","Ztr_t = torch.tensor(Ztr, dtype=torch.float32).to(device)\n","Zte_t = torch.tensor(Zte, dtype=torch.float32).to(device)\n","ytr_t = torch.tensor(ytr, dtype=torch.long).to(device)\n","yte_t = torch.tensor(yte, dtype=torch.long).to(device)\n","\n","idnet = IDInferNet(Ztr.shape[1], len(DRIVERS)).to(device)\n","opt_id = optim.Adam(idnet.parameters(), lr=1e-3)\n","\n","id_train_loss, id_val_acc = [], []\n","\n","start_id = time.time()\n","for ep in range(1, 201):\n","    opt_id.zero_grad()\n","    logits = idnet(Ztr_t)\n","    loss = F.cross_entropy(logits, ytr_t)\n","    loss.backward()\n","    opt_id.step()\n","\n","    with torch.no_grad():\n","        acc = accuracy_score(\n","            yte, idnet(Zte_t).argmax(1).cpu()\n","        )\n","\n","    id_train_loss.append(loss.item())\n","    id_val_acc.append(acc)\n","\n","id_train_time = time.time() - start_id\n","\n","# Predictions\n","with torch.no_grad():\n","    y_pred = idnet(Zte_t).argmax(1).cpu().numpy()\n","    y_prob = F.softmax(idnet(Zte_t), dim=1).cpu().numpy()\n","\n","# Confusion Matrix\n","cm_id = confusion_matrix(yte, y_pred)\n","plt.figure(figsize=(5,4))\n","sns.heatmap(cm_id, annot=True, fmt=\"d\",\n","            xticklabels=le_id.classes_,\n","            yticklabels=le_id.classes_)\n","plt.title(\"IDInferNet Confusion Matrix\")\n","plt.tight_layout()\n","plt.savefig(f\"{DIRS['fig']}/idinfernet_confusion.png\", dpi=300)\n","plt.close()\n","\n","# ROC-AUC\n","from sklearn.preprocessing import label_binarize\n","from sklearn.metrics import roc_curve, auc\n","\n","y_bin = label_binarize(yte, classes=range(len(DRIVERS)))\n","roc_auc = {}\n","\n","plt.figure(figsize=(6,5))\n","for i, cls in enumerate(le_id.classes_):\n","    fpr, tpr, _ = roc_curve(y_bin[:, i], y_prob[:, i])\n","    roc_auc[cls] = auc(fpr, tpr)\n","    plt.plot(fpr, tpr, label=f\"{cls} (AUC={roc_auc[cls]:.3f})\")\n","\n","plt.plot([0,1], [0,1], \"k--\")\n","plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n","plt.title(\"IDInferNet ROC Curves\")\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig(f\"{DIRS['fig']}/idinfernet_roc.png\", dpi=300)\n","plt.close()\n","\n","# ============================================================\n","# 10B. DT-GDIN ‚Äî FULL EVALUATION\n","# ============================================================\n","\n","# Confusion Matrix\n","cm_dt = confusion_matrix(yva, clf(Xv).argmax(1).cpu())\n","plt.figure(figsize=(5,4))\n","sns.heatmap(cm_dt, annot=True, fmt=\"d\",\n","            xticklabels=le.classes_,\n","            yticklabels=le.classes_)\n","plt.title(\"DT-GDIN Confusion Matrix\")\n","plt.tight_layout()\n","plt.savefig(f\"{DIRS['fig']}/dtgdin_confusion.png\", dpi=300)\n","plt.close()\n","\n","# Accuracy curve\n","plt.figure()\n","plt.plot(acc_hist)\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Validation Accuracy\")\n","plt.title(\"DT-GDIN Accuracy Ablation\")\n","plt.tight_layout()\n","plt.savefig(f\"{DIRS['fig']}/dtgdin_accuracy.png\", dpi=300)\n","plt.close()\n","\n","# ============================================================\n","# 11. SAVE EXCEL (MASTER SCHEMA)\n","# ============================================================\n","excel_path = f\"{DIRS['xls']}/SIGNetV1_MasterResults.xlsx\"\n","with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as w:\n","\n","    # SIGNet\n","    pd.DataFrame(hist_base).to_excel(w, \"SIGNet_Baseline_Loss\")\n","    pd.DataFrame(hist_opt).to_excel(w, \"SIGNet_Optimized_Loss\")\n","\n","    # Latent metrics\n","    latent_intra_df.to_excel(w, \"Latent_Intra\", index=False)\n","    latent_inter_df.to_excel(w, \"Latent_Inter\", index=False)\n","    pd.DataFrame({\n","        \"Silhouette\": [sil],\n","        \"Davies_Bouldin\": [db]\n","    }).to_excel(w, \"Cluster_Quality\", index=False)\n","\n","    # IDInferNet\n","    pd.DataFrame({\n","        \"Epoch\": np.arange(1, len(id_train_loss)+1),\n","        \"Train_Loss\": id_train_loss,\n","        \"Val_Accuracy\": id_val_acc\n","    }).to_excel(w, \"IDInferNet_Ablation\", index=False)\n","\n","    pd.DataFrame(\n","        classification_report(yte, y_pred, target_names=le_id.classes_, output_dict=True)\n","    ).T.to_excel(w, \"IDInferNet_Report\")\n","\n","    pd.DataFrame(cm_id).to_excel(w, \"IDInferNet_Confusion\")\n","\n","    # DT-GDIN\n","    pd.DataFrame(acc_hist, columns=[\"DT_GDIN_Val_Accuracy\"]).to_excel(w, \"DT_GDIN_Ablation\", index=False)\n","    pd.DataFrame(cm_dt).to_excel(w, \"DT_GDIN_Confusion\")\n","\n","    # Timing\n","    pd.DataFrame([{\n","        \"SIGNet_Train_Time_s\": None,  # already measured earlier if needed\n","        \"IDInferNet_Train_Time_s\": id_train_time\n","    }]).to_excel(w, \"Timing\", index=False)\n","\n","\n","print(f\"\\n‚úÖ FINAL v1 COMPLETE. Results saved to:\\n{excel_path}\")\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sDUR7HHN8tm","executionInfo":{"status":"ok","timestamp":1765824488916,"user_tz":-300,"elapsed":123547,"user":{"displayName":"Community Of Research & Development","userId":"07614819381775789507"}},"outputId":"7ad41d19-829c-4902-e131-25c040793984"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚öôÔ∏è Using device: cuda\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-57694477.py:464: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(hist_base).to_excel(w, \"SIGNet_Baseline_Loss\")\n","/tmp/ipython-input-57694477.py:465: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(hist_opt).to_excel(w, \"SIGNet_Optimized_Loss\")\n","/tmp/ipython-input-57694477.py:468: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  latent_intra_df.to_excel(w, \"Latent_Intra\", index=False)\n","/tmp/ipython-input-57694477.py:469: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  latent_inter_df.to_excel(w, \"Latent_Inter\", index=False)\n","/tmp/ipython-input-57694477.py:473: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  }).to_excel(w, \"Cluster_Quality\", index=False)\n","/tmp/ipython-input-57694477.py:480: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  }).to_excel(w, \"IDInferNet_Ablation\", index=False)\n","/tmp/ipython-input-57694477.py:484: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  ).T.to_excel(w, \"IDInferNet_Report\")\n","/tmp/ipython-input-57694477.py:486: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(cm_id).to_excel(w, \"IDInferNet_Confusion\")\n","/tmp/ipython-input-57694477.py:489: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(acc_hist, columns=[\"DT_GDIN_Val_Accuracy\"]).to_excel(w, \"DT_GDIN_Ablation\", index=False)\n","/tmp/ipython-input-57694477.py:490: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(cm_dt).to_excel(w, \"DT_GDIN_Confusion\")\n","/tmp/ipython-input-57694477.py:496: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  }]).to_excel(w, \"Timing\", index=False)\n"]},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ FINAL v1 COMPLETE. Results saved to:\n","/content/drive/MyDrive/DT_Driver_Wise_Data/SIGNetV1_Results/Excel/SIGNetV1_MasterResults.xlsx\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"myTMYYBoPRw6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# CVAE_Digital_Twin-v1.3.1\n","# Optimized FP32 + Pruned + Quantized\n","# SIGNet + IDInferNet + DT-GDIN\n","# ============================================================\n","\n","!pip -q install torch torchvision torchaudio thop scikit-learn openpyxl onnx onnxruntime onnxscript\n","\n","# ============================================================\n","# 0. IMPORTS & GLOBALS\n","# ============================================================\n","import os, time, copy\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn.utils.prune as prune\n","\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n","from sklearn.metrics import accuracy_score\n","from thop import profile\n","\n","SEED = 42\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","def ensure_dir(p): os.makedirs(p, exist_ok=True)\n","\n","# ============================================================\n","# 1. DATA & DIRECTORIES\n","# ============================================================\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","DATA_ROOT = \"/content/drive/MyDrive/DT_Driver_Wise_Data\"\n","RES_ROOT  = f\"{DATA_ROOT}/SIGNetV1_Results_v1p3\"\n","\n","DIRS = {\n","    \"fig\": f\"{RES_ROOT}/Figures\",\n","    \"mdl\": f\"{RES_ROOT}/Models\",\n","    \"xls\": f\"{RES_ROOT}/Excel\",\n","    \"onnx\": f\"{RES_ROOT}/ONNX\"\n","}\n","for d in DIRS.values(): ensure_dir(d)\n","\n","DRIVERS = [\"B\", \"D\", \"F\"]\n","FEATURES = [\n","    \"Long_Term_Fuel_Trim_Bank1\",\n","    \"Engine_coolant_temperature.1\",\n","    \"Activation_of_Air_compressor\",\n","    \"Torque_of_friction\",\n","    \"Engine_soacking_time\",\n","    \"Intake_air_pressure\"\n","]\n","TIME_COL = \"Time(s)\"\n","\n","dfs = []\n","for d in DRIVERS:\n","    for s in [\"Train\", \"Valid\"]:\n","        p = f\"{DATA_ROOT}/Driver_{d}_{s}.csv\"\n","        if os.path.exists(p):\n","            df = pd.read_csv(p)\n","            df[\"Driver\"] = d\n","            df[\"Split\"]  = s\n","            dfs.append(df)\n","\n","df_all = pd.concat(dfs, ignore_index=True)\n","df_all = df_all.dropna(subset=FEATURES + [TIME_COL])\n","\n","# ============================================================\n","# 2. PREPROCESSING\n","# ============================================================\n","sc_x, sc_t = StandardScaler(), StandardScaler()\n","enc = OneHotEncoder(sparse_output=False)\n","lab = LabelEncoder()\n","\n","X = sc_x.fit_transform(df_all[FEATURES])\n","T = sc_t.fit_transform(df_all[[TIME_COL]])\n","D = enc.fit_transform(df_all[[\"Driver\"]])\n","y = lab.fit_transform(df_all[\"Driver\"])\n","\n","C = np.concatenate([T, D], axis=1)\n","\n","X = torch.tensor(X, dtype=torch.float32)\n","C = torch.tensor(C, dtype=torch.float32)\n","y = torch.tensor(y, dtype=torch.long)\n","\n","mask_tr = df_all[\"Split\"] == \"Train\"\n","mask_va = df_all[\"Split\"] == \"Valid\"\n","\n","X_tr, C_tr, y_tr = X[mask_tr].to(device), C[mask_tr].to(device), y[mask_tr].to(device)\n","X_va, C_va, y_va = X[mask_va].to(device), C[mask_va].to(device), y[mask_va].to(device)\n","\n","# ============================================================\n","# 3. EDGE PROFILING\n","# ============================================================\n","def edge_profile(model, inputs, dtype_bytes=4):\n","    model.eval()\n","\n","    # --- robust device inference ---\n","    try:\n","        model_device = next(model.parameters()).device\n","    except StopIteration:\n","        # Quantized models have no parameters; they are CPU-only\n","        model_device = torch.device(\"cpu\")\n","\n","    # --- move inputs to same device ---\n","    if isinstance(inputs, tuple):\n","        inputs = tuple(i.to(model_device) for i in inputs)\n","    else:\n","        inputs = inputs.to(model_device)\n","\n","    with torch.no_grad():\n","        flops, params = profile(model, inputs=inputs, verbose=False)\n","\n","        start = time.time()\n","        for _ in range(50):\n","            _ = model(*inputs) if isinstance(inputs, tuple) else model(inputs)\n","\n","        latency = (time.time() - start) / 50 * 1000\n","\n","    return {\n","        \"Params\": int(params),\n","        \"FLOPs\": int(flops),\n","        \"Latency_ms\": latency,\n","        \"Memory_KB\": params * dtype_bytes / 1024,\n","        \"Energy_mJ_est\": flops * 3e-9\n","    }\n","\n","\n","# ============================================================\n","# 4. MODELS\n","# ============================================================\n","class SIGNet(nn.Module):\n","    def __init__(self, x_dim, c_dim, z_dim=8, h=128):\n","        super().__init__()\n","        self.enc = nn.Linear(x_dim + c_dim, h)\n","        self.mu  = nn.Linear(h, z_dim)\n","        self.lv  = nn.Linear(h, z_dim)\n","        self.dec = nn.Linear(z_dim + c_dim, h)\n","        self.out = nn.Linear(h, x_dim)\n","\n","    def encode(self, x, c):\n","        h = F.relu(self.enc(torch.cat([x, c], 1)))\n","        return self.mu(h), self.lv(h)\n","\n","    def reparam(self, mu, lv):\n","        return mu + torch.randn_like(mu) * torch.exp(0.5 * lv)\n","\n","    def decode(self, z, c):\n","        h = F.relu(self.dec(torch.cat([z, c], 1)))\n","        return self.out(h)\n","\n","    def forward(self, x, c):\n","        mu, lv = self.encode(x, c)\n","        z = self.reparam(mu, lv)\n","        return self.decode(z, c), mu, lv\n","\n","class IDInferNet(nn.Module):\n","    def __init__(self, z, n):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(z,128), nn.ReLU(),\n","            nn.Linear(128,n)\n","        )\n","    def forward(self, z): return self.net(z)\n","\n","class DT_GDIN(nn.Module):\n","    def __init__(self, d, n):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(d,256), nn.ReLU(),\n","            nn.Linear(256,n)\n","        )\n","    def forward(self, x): return self.net(x)\n","\n","# ============================================================\n","# 5. TRAINING\n","# ============================================================\n","def vae_loss(r, x, mu, lv):\n","    return F.mse_loss(r, x) - 0.5 * torch.mean(1 + lv - mu**2 - lv.exp())\n","\n","def train_signet(lr=5e-4, epochs=300):\n","    model = SIGNet(X.shape[1], C.shape[1]).to(device)\n","    opt = optim.Adam(model.parameters(), lr=lr)\n","    best, best_wts = 1e9, None\n","\n","    for _ in range(epochs):\n","        opt.zero_grad()\n","        r, mu, lv = model(X_tr, C_tr)\n","        loss = vae_loss(r, X_tr, mu, lv)\n","        loss.backward()\n","        opt.step()\n","\n","        with torch.no_grad():\n","            rv, mv, lv_v = model(X_va, C_va)\n","            vloss = vae_loss(rv, X_va, mv, lv_v)\n","\n","        if vloss < best:\n","            best, best_wts = vloss, copy.deepcopy(model.state_dict())\n","\n","    model.load_state_dict(best_wts)\n","    return model\n","\n","sig_fp32 = train_signet()\n","\n","with torch.no_grad():\n","    Z_tr = sig_fp32.encode(X_tr, C_tr)[0]\n","    Z_va = sig_fp32.encode(X_va, C_va)[0]\n","\n","idnet = IDInferNet(Z_tr.shape[1], len(DRIVERS)).to(device)\n","gdnet = DT_GDIN(X_tr.shape[1], len(DRIVERS)).to(device)\n","\n","def train_cls(model, X, y, epochs=200):\n","    opt = optim.Adam(model.parameters(), 1e-3)\n","    for _ in range(epochs):\n","        opt.zero_grad()\n","        loss = F.cross_entropy(model(X), y)\n","        loss.backward()\n","        opt.step()\n","    return model\n","\n","idnet = train_cls(idnet, Z_tr, y_tr)\n","gdnet = train_cls(gdnet, X_tr, y_tr)\n","\n","# ============================================================\n","# 6. PRUNING & QUANTIZATION\n","# ============================================================\n","def prune_model(model, amount=0.3):\n","    for m in model.modules():\n","        if isinstance(m, nn.Linear):\n","            prune.ln_structured(m, \"weight\", amount=amount, n=1, dim=0)\n","            prune.remove(m, \"weight\")\n","    return model\n","\n","def quant_model(model):\n","    return torch.quantization.quantize_dynamic(\n","        model.cpu(), {nn.Linear}, dtype=torch.qint8\n","    )\n","\n","models = {\n","    \"SIGNet_FP32\": sig_fp32,\n","    \"SIGNet_Pruned\": prune_model(copy.deepcopy(sig_fp32)),\n","    \"SIGNet_Quant\": quant_model(sig_fp32),\n","\n","    \"IDInferNet_FP32\": idnet,\n","    \"IDInferNet_Pruned\": prune_model(copy.deepcopy(idnet)),\n","    \"IDInferNet_Quant\": quant_model(idnet),\n","\n","    \"DT_GDIN_FP32\": gdnet,\n","    \"DT_GDIN_Pruned\": prune_model(copy.deepcopy(gdnet)),\n","    \"DT_GDIN_Quant\": quant_model(gdnet),\n","}\n","\n","# ============================================================\n","# 7. METRICS + SAVING\n","# ============================================================\n","rows = []\n","for name, m in models.items():\n","    is_quant = \"Quant\" in name\n","\n","    # Quantized models ‚Üí CPU only\n","    if is_quant:\n","        m = m.cpu()\n","    else:\n","        m = m.to(device)\n","\n","    inputs = (\n","        (X_va[:1], C_va[:1]) if \"SIGNet\" in name else\n","        (Z_va[:1] if \"IDInferNet\" in name else X_va[:1],)\n","    )\n","\n","    rows.append({\n","        \"Model\": name,\n","        **edge_profile(\n","            m,\n","            inputs,\n","            dtype_bytes=1 if is_quant else 4\n","        )\n","    })\n","\n","    torch.save(m.state_dict(), f\"{DIRS['mdl']}/{name}.pt\")\n","\n","\n","df_metrics = pd.DataFrame(rows)\n","df_metrics.to_excel(f\"{DIRS['xls']}/Edge_Profiles_All_v1p3.xlsx\", index=False)\n","\n","# ============================================================\n","# 8. ONNX (FP32 ONLY)\n","# ============================================================\n","torch.onnx.export(\n","    sig_fp32.cpu().eval(),\n","    (X_va[:1].cpu(), C_va[:1].cpu()),\n","    f\"{DIRS['onnx']}/SIGNet_FP32.onnx\",\n","    opset_version=18,\n","    input_names=[\"X\",\"C\"],\n","    output_names=[\"X_hat\",\"mu\",\"lv\"],\n","    dynamic_axes={\"X\":{0:\"batch\"}, \"C\":{0:\"batch\"}}\n",")\n","\n","print(\"‚úÖ v1.3.1 COMPLETE\")\n","print(\"Saved Models:\", DIRS[\"mdl\"])\n","print(\"Saved Metrics:\", DIRS[\"xls\"])\n","print(\"Saved ONNX:\", DIRS[\"onnx\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJ-1ZnuIvg0n","executionInfo":{"status":"ok","timestamp":1765989654955,"user_tz":-300,"elapsed":23844,"user":{"displayName":"Community Of Research & Development","userId":"07614819381775789507"}},"outputId":"7fd42707-ccdf-4c83-dcb0-bb01c4d9e655"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/693.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m686.1/693.4 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m693.4/693.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDevice: cuda\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4037970541.py:242: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n","For migrations of users: \n","1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n","2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n","3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n","see https://github.com/pytorch/ao/issues/2259 for more details\n","  return torch.quantization.quantize_dynamic(\n","/tmp/ipython-input-4037970541.py:296: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n","  torch.onnx.export(\n"]},{"output_type":"stream","name":"stdout","text":["[torch.onnx] Obtain model graph for `SIGNet([...]` with `torch.export.export(..., strict=False)`...\n","[torch.onnx] Obtain model graph for `SIGNet([...]` with `torch.export.export(..., strict=False)`... ‚úÖ\n","[torch.onnx] Run decomposition...\n","[torch.onnx] Run decomposition... ‚úÖ\n","[torch.onnx] Translate the graph into ONNX...\n","[torch.onnx] Translate the graph into ONNX... ‚úÖ\n","‚úÖ v1.3.1 COMPLETE\n","Saved Models: /content/drive/MyDrive/DT_Driver_Wise_Data/SIGNetV1_Results_v1p3/Models\n","Saved Metrics: /content/drive/MyDrive/DT_Driver_Wise_Data/SIGNetV1_Results_v1p3/Excel\n","Saved ONNX: /content/drive/MyDrive/DT_Driver_Wise_Data/SIGNetV1_Results_v1p3/ONNX\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"7K6lLw364Hlu"},"execution_count":null,"outputs":[]}]}