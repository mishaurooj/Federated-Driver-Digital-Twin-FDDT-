{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Jmc1ngD3HeOl8iMhjO5aLujA4iMpyLKY","timestamp":1765623213938}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<style>\n",".arch-table {\n","  border-collapse: collapse;\n","  width: 100%;\n","  font-family: Arial, sans-serif;\n","  font-size: 14px;\n","}\n","\n",".arch-table th {\n","  background-color: #1f4fd8;\n","  color: #ffffff;\n","  padding: 10px;\n","  border: 1px solid #333;\n","  text-align: center;\n","}\n","\n",".arch-table td {\n","  padding: 10px;\n","  border: 1px solid #333;\n","  vertical-align: top;\n","}\n","\n",".arch-table tr:nth-child(even) {\n","  background-color: #f4f6fb;\n","}\n","\n",".arch-table .v1 {\n","  background-color: #e3f2fd;\n","}\n","\n",".arch-table .v2 {\n","  background-color: #e8f5e9;\n","}\n","\n",".arch-title {\n","  font-family: Arial, sans-serif;\n","  font-size: 18px;\n","  font-weight: bold;\n","  margin-bottom: 10px;\n","  color: #1f4fd8;\n","}\n","\n",".arch-subtitle {\n","  font-size: 13px;\n","  margin-bottom: 16px;\n","  color: #333;\n","}\n","</style>\n","\n","<div class=\"arch-title\">\n","SIGNet Architecture Comparison (v1 vs v2)\n","</div>\n","\n","<div class=\"arch-subtitle\">\n","Comparison of baseline and enhanced digital-twin architectures used for driver identification.\n","</div>\n","\n","<table class=\"arch-table\">\n","  <tr>\n","    <th>Aspect</th>\n","    <th>SIGNet-v1 (Baseline)</th>\n","    <th>SIGNet-v2 (Enhanced)</th>\n","  </tr>\n","\n","  <tr>\n","    <td><b>Design Objective</b></td>\n","    <td class=\"v1\">\n","      Proof-of-concept conditional VAE for driver-wise digital twin generation.\n","    </td>\n","    <td class=\"v2\">\n","      Robust, deployment-ready digital twin with improved stability, separability, and generalization.\n","    </td>\n","  </tr>\n","\n","  <tr>\n","    <td><b>Encoder Depth</b></td>\n","    <td class=\"v1\">\n","      Single hidden layer (shallow encoder).\n","    </td>\n","    <td class=\"v2\">\n","      Multi-layer encoder with increased representational capacity.\n","    </td>\n","  </tr>\n","\n","  <tr>\n","    <td><b>Decoder Depth</b></td>\n","    <td class=\"v1\">\n","      Symmetric shallow decoder.\n","    </td>\n","    <td class=\"v2\">\n","      Deeper decoder enabling higher-fidelity signal reconstruction and generation.\n","    </td>\n","  </tr>\n","\n","  <tr>\n","    <td><b>Latent Dimension</b></td>\n","    <td class=\"v1\">\n","      Low-dimensional latent space (compact but limited separability).\n","    </td>\n","    <td class=\"v2\">\n","      Higher-capacity latent space enabling improved inter-driver discrimination.\n","    </td>\n","  </tr>\n","\n","  <tr>\n","    <td><b>Stabilization Techniques</b></td>\n","    <td class=\"v1\">\n","      None (plain fully-connected layers).\n","    </td>\n","    <td class=\"v2\">\n","      Batch normalization and deeper nonlinear transformations.\n","    </td>\n","  </tr>\n","\n","  <tr>\n","    <td><b>Loss Function</b></td>\n","    <td class=\"v1\">\n","      Standard VAE loss (Reconstruction + KL divergence).\n","    </td>\n","    <td class=\"v2\">\n","      Weighted VAE loss (Œ≤-VAE style) for improved latent regularization.\n","    </td>\n","  </tr>\n","\n","  <tr>\n","    <td><b>Training Strategy</b></td>\n","    <td class=\"v1\">\n","      Fixed learning rate, early stopping based on validation loss.\n","    </td>\n","    <td class=\"v2\">\n","      Optimized learning rate, extended patience, and stability-oriented early stopping.\n","    </td>\n","  </tr>\n","\n","  <tr>\n","    <td><b>Latent Consistency Metrics</b></td>\n","    <td class=\"v1\">\n","      Limited intra-class similarity metrics.\n","    </td>\n","    <td class=\"v2\">\n","      Full IEEE-grade suite: cosine similarity, Euclidean spread, variance trace, coefficient of variation.\n","    </td>\n","  </tr>\n","\n","  <tr>\n","    <td><b>Downstream Usage</b></td>\n","    <td class=\"v1\">\n","      Latent features used for IDInferNet and DT-GDIN (baseline performance).\n","    </td>\n","    <td class=\"v2\">\n","      Stronger latent embeddings yielding improved classification accuracy and robustness.\n","    </td>\n","  </tr>\n","\n","  <tr>\n","    <td><b>Generalization Capability</b></td>\n","    <td class=\"v1\">\n","      Moderate ‚Äî sensitive to noise and distribution shifts.\n","    </td>\n","    <td class=\"v2\">\n","      High ‚Äî improved robustness for synthetic generation and edge inference.\n","    </td>\n","  </tr>\n","\n","  </tr>\n","\n","</table>\n"],"metadata":{"id":"2MA1zpr5eFRA"}},{"cell_type":"markdown","source":["![download.svg](data:image/svg+xml;base64,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)"],"metadata":{"id":"ItsOMw4qiv74"}},{"cell_type":"markdown","source":["![download (2).svg](data:image/svg+xml;base64,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)"],"metadata":{"id":"1xox8HANi0Dk"}},{"cell_type":"markdown","source":["IDInferNet-v2 Architecture (Latent Classifier)\n","![download (3).svg](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iOTAwIiBoZWlnaHQ9IjI2MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCjxzdHlsZT4KdGV4dCB7IGZvbnQtZmFtaWx5OiBBcmlhbDsgZm9udC1zaXplOjE0cHg7IH0KLmJsb2NrIHsgc3Ryb2tlOiMzMzM7IHN0cm9rZS13aWR0aDoxLjU7IHJ4OjY7IHJ5OjY7IH0KLmFycm93IHsgc3Ryb2tlOiMzMzM7IHN0cm9rZS13aWR0aDoxLjU7IGZpbGw6bm9uZTsgfQo8L3N0eWxlPgoKPGRlZnM+CjxtYXJrZXIgaWQ9ImEiIG1hcmtlcldpZHRoPSIxMCIgbWFya2VySGVpZ2h0PSIxMCIgcmVmWD0iOSIgcmVmWT0iMyIgb3JpZW50PSJhdXRvIj4KPHBhdGggZD0iTTAsMCBMMCw2IEw5LDMgeiIgZmlsbD0iIzMzMyI+PC9wYXRoPgo8L21hcmtlcj4KPC9kZWZzPgoKPHJlY3QgY2xhc3M9ImJsb2NrIiB4PSI0MCIgeT0iMTAwIiB3aWR0aD0iMjAwIiBoZWlnaHQ9IjYwIiBmaWxsPSIjZDFjNGU5Ij48L3JlY3Q+Cjx0ZXh0IHg9IjE0MCIgeT0iMTM1IiB0ZXh0LWFuY2hvcj0ibWlkZGxlIj5MYXRlbnQgejwvdGV4dD4KCjxyZWN0IGNsYXNzPSJibG9jayIgeD0iMzIwIiB5PSIxMDAiIHdpZHRoPSIyMjAiIGhlaWdodD0iNjAiIGZpbGw9IiNmZmUwYjIiPjwvcmVjdD4KPHRleHQgeD0iNDMwIiB5PSIxMzUiIHRleHQtYW5jaG9yPSJtaWRkbGUiPkZDICsgRHJvcG91dDwvdGV4dD4KCjxyZWN0IGNsYXNzPSJibG9jayIgeD0iNjIwIiB5PSIxMDAiIHdpZHRoPSIyMjAiIGhlaWdodD0iNjAiIGZpbGw9IiNhNWQ2YTciPjwvcmVjdD4KPHRleHQgeD0iNzMwIiB5PSIxMzUiIHRleHQtYW5jaG9yPSJtaWRkbGUiPkRyaXZlciBJRDwvdGV4dD4KCjxsaW5lIGNsYXNzPSJhcnJvdyIgeDE9IjI0MCIgeTE9IjEzMCIgeDI9IjMyMCIgeTI9IjEzMCIgbWFya2VyLWVuZD0idXJsKCNhKSI+PC9saW5lPgo8bGluZSBjbGFzcz0iYXJyb3ciIHgxPSI1NDAiIHkxPSIxMzAiIHgyPSI2MjAiIHkyPSIxMzAiIG1hcmtlci1lbmQ9InVybCgjYSkiPjwvbGluZT4KCjwvc3ZnPgo=)"],"metadata":{"id":"upu6fVCajamu"}},{"cell_type":"markdown","source":["DT-GDIN-v2 Architecture (Digital-Twin Generated Classifier)\n","![download (4).svg](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iOTAwIiBoZWlnaHQ9IjMwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCjxzdHlsZT4KdGV4dCB7IGZvbnQtZmFtaWx5OiBBcmlhbDsgZm9udC1zaXplOjE0cHg7IH0KLmJsb2NrIHsgc3Ryb2tlOiMzMzM7IHN0cm9rZS13aWR0aDoxLjU7IHJ4OjY7IHJ5OjY7IH0KLmFycm93IHsgc3Ryb2tlOiMzMzM7IHN0cm9rZS13aWR0aDoxLjU7IGZpbGw6bm9uZTsgfQo8L3N0eWxlPgoKPGRlZnM+CjxtYXJrZXIgaWQ9ImEiIG1hcmtlcldpZHRoPSIxMCIgbWFya2VySGVpZ2h0PSIxMCIgcmVmWD0iOSIgcmVmWT0iMyIgb3JpZW50PSJhdXRvIj4KPHBhdGggZD0iTTAsMCBMMCw2IEw5LDMgeiIgZmlsbD0iIzMzMyI+PC9wYXRoPgo8L21hcmtlcj4KPC9kZWZzPgoKPHJlY3QgY2xhc3M9ImJsb2NrIiB4PSI0MCIgeT0iMTIwIiB3aWR0aD0iMjQwIiBoZWlnaHQ9IjYwIiBmaWxsPSIjYzhlNmM5Ij48L3JlY3Q+Cjx0ZXh0IHg9IjE2MCIgeT0iMTU1IiB0ZXh0LWFuY2hvcj0ibWlkZGxlIj5HZW5lcmF0ZWQgU2lnbmFsczwvdGV4dD4KCjxyZWN0IGNsYXNzPSJibG9jayIgeD0iMzYwIiB5PSIxMjAiIHdpZHRoPSIyMjAiIGhlaWdodD0iNjAiIGZpbGw9IiNmZmUwYjIiPjwvcmVjdD4KPHRleHQgeD0iNDcwIiB5PSIxNTUiIHRleHQtYW5jaG9yPSJtaWRkbGUiPkZDICsgUmVMVTwvdGV4dD4KCjxyZWN0IGNsYXNzPSJibG9jayIgeD0iNjYwIiB5PSIxMjAiIHdpZHRoPSIyMjAiIGhlaWdodD0iNjAiIGZpbGw9IiNhNWQ2YTciPjwvcmVjdD4KPHRleHQgeD0iNzcwIiB5PSIxNTUiIHRleHQtYW5jaG9yPSJtaWRkbGUiPkRyaXZlciBJRDwvdGV4dD4KCjxsaW5lIGNsYXNzPSJhcnJvdyIgeDE9IjI4MCIgeTE9IjE1MCIgeDI9IjM2MCIgeTI9IjE1MCIgbWFya2VyLWVuZD0idXJsKCNhKSI+PC9saW5lPgo8bGluZSBjbGFzcz0iYXJyb3ciIHgxPSI1ODAiIHkxPSIxNTAiIHgyPSI2NjAiIHkyPSIxNTAiIG1hcmtlci1lbmQ9InVybCgjYSkiPjwvbGluZT4KCjwvc3ZnPgo=)"],"metadata":{"id":"6ffkI81djfMK"}},{"cell_type":"code","source":["# ============================================================\n","# CVAE_Digital_Twin-v2.ipynb\n","# SIGNet-v2 + IDInferNet-v2 + DT-GDIN-v2\n","# IEEE Transactions‚ÄìGrade | FULL | REPRODUCIBLE\n","# ============================================================\n","\n","!pip -q install torch torchvision torchaudio thop seaborn scikit-learn openpyxl\n","\n","# ============================================================\n","# 1. IMPORTS & GLOBAL CONFIG\n","# ============================================================\n","\n","import os, time, copy, json, math\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    accuracy_score, classification_report,\n","    confusion_matrix, roc_curve, auc\n",")\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.metrics import silhouette_score, davies_bouldin_score\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","from scipy.spatial.distance import euclidean\n","from thop import profile\n","\n","SEED = 42\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"‚öôÔ∏è Using device: {device}\")\n","\n","# ============================================================\n","# 2. DATA & DIRECTORY SETUP\n","# ============================================================\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","DATA_ROOT = \"/content/drive/MyDrive/DT_Driver_Wise_Data\"\n","RES_ROOT  = f\"{DATA_ROOT}/SIGNetV2_Results\"\n","\n","DIRS = {\n","    \"fig\": f\"{RES_ROOT}/Figures\",\n","    \"mdl\": f\"{RES_ROOT}/Models\",\n","    \"xls\": f\"{RES_ROOT}/Excel\",\n","}\n","for d in DIRS.values():\n","    os.makedirs(d, exist_ok=True)\n","\n","DRIVERS = [\"B\", \"D\", \"F\"]\n","\n","FEATURES = [\n","    \"Long_Term_Fuel_Trim_Bank1\",\n","    \"Engine_coolant_temperature.1\",\n","    \"Activation_of_Air_compressor\",\n","    \"Torque_of_friction\",\n","    \"Engine_soacking_time\",\n","    \"Intake_air_pressure\",\n","]\n","TIME_COL = \"Time(s)\"\n","\n","# ============================================================\n","# 3. DATA LOADING\n","# ============================================================\n","\n","dfs = []\n","for drv in DRIVERS:\n","    for split in [\"Train\", \"Valid\"]:\n","        p = f\"{DATA_ROOT}/Driver_{drv}_{split}.csv\"\n","        if os.path.exists(p):\n","            df = pd.read_csv(p)\n","            df[\"Driver\"] = drv\n","            df[\"Split\"] = split\n","            dfs.append(df)\n","\n","df_all = pd.concat(dfs, ignore_index=True)\n","df_all = df_all.dropna(subset=FEATURES + [TIME_COL])\n","\n","# ============================================================\n","# 4. PREPROCESSING\n","# ============================================================\n","\n","sc_x = StandardScaler()\n","sc_t = StandardScaler()\n","enc  = OneHotEncoder(sparse_output=False)\n","\n","X = sc_x.fit_transform(df_all[FEATURES])\n","T = sc_t.fit_transform(df_all[[TIME_COL]])\n","D = enc.fit_transform(df_all[[\"Driver\"]])\n","C = np.concatenate([T, D], axis=1)\n","\n","X = torch.tensor(X, dtype=torch.float32).to(device)\n","C = torch.tensor(C, dtype=torch.float32).to(device)\n","\n","mask_tr = df_all[\"Split\"] == \"Train\"\n","mask_va = df_all[\"Split\"] == \"Valid\"\n","\n","X_tr, C_tr = X[mask_tr], C[mask_tr]\n","X_va, C_va = X[mask_va], C[mask_va]\n","drv_va     = df_all.loc[mask_va, \"Driver\"].values\n","\n","# ============================================================\n","# 5. SIGNET-V2 ARCHITECTURE (IMPROVED CAPACITY + STABILITY)\n","# ============================================================\n","\n","class SIGNetV2(nn.Module):\n","    def __init__(self, x_dim, c_dim, z_dim=16):\n","        super().__init__()\n","\n","        self.encoder = nn.Sequential(\n","            nn.Linear(x_dim + c_dim, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","            nn.Linear(256, 128),\n","            nn.ReLU()\n","        )\n","        self.mu = nn.Linear(128, z_dim)\n","        self.lv = nn.Linear(128, z_dim)\n","\n","        self.decoder = nn.Sequential(\n","            nn.Linear(z_dim + c_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, x_dim)\n","        )\n","\n","    def encode(self, x, c):\n","        h = self.encoder(torch.cat([x, c], 1))\n","        return self.mu(h), self.lv(h)\n","\n","    def reparam(self, mu, lv):\n","        return mu + torch.randn_like(mu) * torch.exp(0.5 * lv)\n","\n","    def decode(self, z, c):\n","        return self.decoder(torch.cat([z, c], 1))\n","\n","    def forward(self, x, c):\n","        mu, lv = self.encode(x, c)\n","        z = self.reparam(mu, lv)\n","        return self.decode(z, c), mu, lv\n","\n","def vae_loss(xr, x, mu, lv, beta=1.0):\n","    rec = F.mse_loss(xr, x)\n","    kl  = -0.5 * torch.mean(1 + lv - mu**2 - lv.exp())\n","    return rec + beta * kl, rec, kl\n","\n","# ============================================================\n","# 6. TRAIN SIGNET-V2 (EARLY STOPPING + TIMING)\n","# ============================================================\n","\n","def train_signet_v2(lr):\n","    model = SIGNetV2(X.shape[1], C.shape[1]).to(device)\n","    opt = optim.Adam(model.parameters(), lr=lr)\n","\n","    hist = {\"train\": [], \"val\": [], \"rec\": [], \"kl\": []}\n","    best, patience = np.inf, 0\n","\n","    t0 = time.time()\n","\n","    for ep in range(1, 501):\n","        model.train()\n","        opt.zero_grad()\n","\n","        xr, mu, lv = model(X_tr, C_tr)\n","        loss, rec, kl = vae_loss(xr, X_tr, mu, lv, beta=0.5)\n","        loss.backward()\n","        opt.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            xv, mu_v, lv_v = model(X_va, C_va)\n","            vloss, _, _ = vae_loss(xv, X_va, mu_v, lv_v, beta=0.5)\n","\n","        hist[\"train\"].append(loss.item())\n","        hist[\"val\"].append(vloss.item())\n","        hist[\"rec\"].append(rec.item())\n","        hist[\"kl\"].append(kl.item())\n","\n","        if vloss < best - 1e-4:\n","            best = vloss\n","            best_wts = copy.deepcopy(model.state_dict())\n","            patience = 0\n","        else:\n","            patience += 1\n","\n","        if patience >= 30:\n","            break\n","\n","    train_time = time.time() - t0\n","    model.load_state_dict(best_wts)\n","    return model, hist, train_time\n","\n","model_v2, hist_v2, signet_train_time = train_signet_v2(1e-3)\n","torch.save(model_v2.state_dict(), f\"{DIRS['mdl']}/signet_v2.pt\")\n","\n","# ============================================================\n","# 7. LATENT EXTRACTION\n","# ============================================================\n","\n","model_v2.eval()\n","with torch.no_grad():\n","    Z_va = model_v2.encode(X_va, C_va)[0].cpu().numpy()\n","\n","# ============================================================\n","# 8. LATENT CONSISTENCY METRICS (IEEE EXTENDED)\n","# ============================================================\n","\n","latent_rows = []\n","for d in DRIVERS:\n","    Zd = Z_va[drv_va == d]\n","\n","    latent_rows.append({\n","        \"Driver\": d,\n","        \"Cosine_Sim\": cosine_similarity(Zd).mean(),\n","        \"Mean_Euclid\": np.mean([\n","            euclidean(Zd[i], Zd[j])\n","            for i in range(len(Zd)) for j in range(i+1, len(Zd))\n","        ]),\n","        \"Var_Trace\": np.trace(np.cov(Zd.T)),\n","        \"Coeff_Var\": np.std(Zd) / (np.mean(np.abs(Zd)) + 1e-8)\n","    })\n","\n","latent_intra_df = pd.DataFrame(latent_rows)\n","\n","sil = silhouette_score(Z_va, LabelEncoder().fit_transform(drv_va))\n","db  = davies_bouldin_score(Z_va, LabelEncoder().fit_transform(drv_va))\n","\n","# ============================================================\n","# 9. IDINFERNET-V2 (REAL LATENT CLASSIFIER)\n","# ============================================================\n","\n","class IDInferNetV2(nn.Module):\n","    def __init__(self, z_dim, n):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(z_dim, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(128, n)\n","        )\n","    def forward(self, z): return self.net(z)\n","\n","le = LabelEncoder()\n","y = le.fit_transform(drv_va)\n","\n","Ztr, Zte, ytr, yte = train_test_split(\n","    Z_va, y, stratify=y, test_size=0.3, random_state=SEED\n",")\n","\n","Ztr = torch.tensor(Ztr, dtype=torch.float32).to(device)\n","Zte = torch.tensor(Zte, dtype=torch.float32).to(device)\n","ytr = torch.tensor(ytr, dtype=torch.long).to(device)\n","yte = torch.tensor(yte, dtype=torch.long).to(device)\n","\n","idnet = IDInferNetV2(Ztr.shape[1], len(DRIVERS)).to(device)\n","opt = optim.Adam(idnet.parameters(), lr=1e-3)\n","\n","acc_hist, loss_hist = [], []\n","t0 = time.time()\n","\n","for ep in range(1, 301):\n","    opt.zero_grad()\n","    loss = F.cross_entropy(idnet(Ztr), ytr)\n","    loss.backward()\n","    opt.step()\n","\n","    with torch.no_grad():\n","        acc = accuracy_score(yte.cpu(), idnet(Zte).argmax(1).cpu())\n","    acc_hist.append(acc)\n","    loss_hist.append(loss.item())\n","\n","idinfer_train_time = time.time() - t0\n","torch.save(idnet.state_dict(), f\"{DIRS['mdl']}/idinfernet_v2.pt\")\n","\n","# ============================================================\n","# 9A. INTER-DRIVER LATENT SEPARABILITY METRICS\n","# ============================================================\n","\n","centroids = {}\n","for d in DRIVERS:\n","    centroids[d] = Z_va[drv_va == d].mean(axis=0)\n","\n","inter_rows = []\n","for i, d1 in enumerate(DRIVERS):\n","    for d2 in DRIVERS[i+1:]:\n","        c1, c2 = centroids[d1], centroids[d2]\n","        inter_rows.append({\n","            \"Pair\": f\"{d1}-{d2}\",\n","            \"Centroid_Euclidean\": euclidean(c1, c2),\n","            \"Centroid_Cosine_Distance\": 1 - cosine_similarity([c1],[c2])[0,0],\n","            \"Fisher_Ratio\":\n","                np.linalg.norm(c1-c2)**2 /\n","                (np.var(Z_va[drv_va==d1]) + np.var(Z_va[drv_va==d2]) + 1e-8)\n","        })\n","\n","latent_inter_df = pd.DataFrame(inter_rows)\n","\n","# ============================================================\n","# 9B. LATENT SPACE VISUALIZATION\n","# ============================================================\n","\n","pca = PCA(n_components=2)\n","Z_pca = pca.fit_transform(Z_va)\n","\n","tsne = TSNE(n_components=2, perplexity=30, random_state=SEED)\n","Z_tsne = tsne.fit_transform(Z_va)\n","\n","plt.figure(figsize=(6,5))\n","sns.scatterplot(x=Z_pca[:,0], y=Z_pca[:,1], hue=drv_va)\n","plt.title(\"SIGNet-v2 Latent Space (PCA)\")\n","plt.tight_layout()\n","plt.savefig(f\"{DIRS['fig']}/latent_pca.png\", dpi=300)\n","plt.close()\n","\n","plt.figure(figsize=(6,5))\n","sns.scatterplot(x=Z_tsne[:,0], y=Z_tsne[:,1], hue=drv_va)\n","plt.title(\"SIGNet-v2 Latent Space (t-SNE)\")\n","plt.tight_layout()\n","plt.savefig(f\"{DIRS['fig']}/latent_tsne.png\", dpi=300)\n","plt.close()\n","\n","# ============================================================\n","# 9C. EDGE DEPLOYMENT METRICS\n","# ============================================================\n","\n","def latency_ms(model):\n","    model.eval()\n","    with torch.no_grad():\n","        t0 = time.time()\n","        for _ in range(100):\n","            _ = model(X_va[:1], C_va[:1])\n","    return (time.time() - t0)/100 * 1000\n","\n","latency = latency_ms(model_v2)\n","flops, params = profile(model_v2, inputs=(X_va[:1], C_va[:1]), verbose=False)\n","\n","edge_df = pd.DataFrame([{\n","    \"Latency_ms\": latency,\n","    \"FLOPs_M\": flops/1e6,\n","    \"Params_M\": params/1e6\n","}])\n","\n","# ============================================================\n","# 10. DT-GDIN-V2 (DIGITAL-TWIN GENERATED CLASSIFIER)\n","# ============================================================\n","\n","signatures = {}\n","for d in DRIVERS:\n","    idx = df_all[\"Driver\"] == d\n","    with torch.no_grad():\n","        mu, _ = model_v2.encode(X[idx], C[idx])\n","        signatures[d] = mu.mean(0)\n","\n","Xs, ys = [], []\n","for d in DRIVERS:\n","    mu = signatures[d]\n","    Cd = C[df_all[\"Driver\"] == d]\n","    for _ in range(2500):\n","        z = mu + 0.7 * torch.randn_like(mu)\n","        c = Cd[np.random.randint(len(Cd))]\n","        with torch.no_grad():\n","            xg = model_v2.decode(z.unsqueeze(0), c.unsqueeze(0))\n","        Xs.append(xg.cpu().numpy()[0])\n","        ys.append(d)\n","\n","Xs = np.array(Xs)\n","ys = le.fit_transform(ys)\n","\n","Xtr, Xva, ytr, yva = train_test_split(\n","    Xs, ys, stratify=ys, test_size=0.3, random_state=SEED\n",")\n","\n","class DTGDINV2(nn.Module):\n","    def __init__(self, d, n):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(d, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, n)\n","        )\n","    def forward(self, x): return self.net(x)\n","\n","dtgdin = DTGDINV2(Xtr.shape[1], len(DRIVERS)).to(device)\n","opt = optim.Adam(dtgdin.parameters(), lr=1e-3)\n","\n","Xt = torch.tensor(Xtr, dtype=torch.float32).to(device)\n","yt = torch.tensor(ytr, dtype=torch.long).to(device)\n","Xv = torch.tensor(Xva, dtype=torch.float32).to(device)\n","yv = torch.tensor(yva, dtype=torch.long).to(device)\n","\n","dt_acc = []\n","t0 = time.time()\n","\n","for ep in range(1, 251):\n","    opt.zero_grad()\n","    loss = F.cross_entropy(dtgdin(Xt), yt)\n","    loss.backward()\n","    opt.step()\n","\n","    with torch.no_grad():\n","        acc = accuracy_score(yv.cpu(), dtgdin(Xv).argmax(1).cpu())\n","    dt_acc.append(acc)\n","\n","dtgdin_train_time = time.time() - t0\n","torch.save(dtgdin.state_dict(), f\"{DIRS['mdl']}/dtgdin_v2.pt\")\n","\n","# ============================================================\n","# 10A. IDInferNet-v2 FULL EVALUATION\n","# ============================================================\n","\n","y_pred = idnet(Zte).argmax(1).cpu().numpy()\n","y_prob = F.softmax(idnet(Zte), dim=1).detach().cpu().numpy()\n","\n","\n","cm_id = confusion_matrix(yte.cpu(), y_pred)\n","\n","plt.figure(figsize=(5,4))\n","sns.heatmap(cm_id, annot=True, fmt=\"d\",\n","            xticklabels=le.classes_,\n","            yticklabels=le.classes_)\n","plt.title(\"IDInferNet-v2 Confusion Matrix\")\n","plt.tight_layout()\n","plt.savefig(f\"{DIRS['fig']}/idinfernet_v2_cm.png\", dpi=300)\n","plt.close()\n","\n","# ROC\n","from sklearn.preprocessing import label_binarize\n","y_bin = label_binarize(yte.cpu(), classes=range(len(DRIVERS)))\n","\n","roc_rows = []\n","plt.figure(figsize=(6,5))\n","for i, cls in enumerate(le.classes_):\n","    fpr, tpr, _ = roc_curve(y_bin[:,i], y_prob[:,i])\n","    auc_i = auc(fpr, tpr)\n","    roc_rows.append({\"Driver\": cls, \"AUC\": auc_i})\n","    plt.plot(fpr, tpr, label=f\"{cls} (AUC={auc_i:.3f})\")\n","\n","plt.plot([0,1],[0,1],'k--')\n","plt.legend(); plt.title(\"IDInferNet-v2 ROC\")\n","plt.tight_layout()\n","plt.savefig(f\"{DIRS['fig']}/idinfernet_v2_roc.png\", dpi=300)\n","plt.close()\n","\n","id_report_df = pd.DataFrame(\n","    classification_report(yte.cpu(), y_pred, target_names=le.classes_, output_dict=True)\n",").T\n","\n","# ============================================================\n","# 10B. DT-GDIN-v2 FULL EVALUATION\n","# ============================================================\n","\n","dt_pred = dtgdin(Xv).argmax(1).cpu().numpy()\n","cm_dt = confusion_matrix(yv.cpu(), dt_pred)\n","\n","plt.figure(figsize=(5,4))\n","sns.heatmap(cm_dt, annot=True, fmt=\"d\",\n","            xticklabels=le.classes_,\n","            yticklabels=le.classes_)\n","plt.title(\"DT-GDIN-v2 Confusion Matrix\")\n","plt.tight_layout()\n","plt.savefig(f\"{DIRS['fig']}/dtgdin_v2_cm.png\", dpi=300)\n","plt.close()\n","\n","dt_report_df = pd.DataFrame(\n","    classification_report(yv.cpu(), dt_pred, target_names=le.classes_, output_dict=True)\n",").T\n","\n","# ============================================================\n","# 11. SAVE FIGURES\n","# ============================================================\n","\n","plt.figure()\n","plt.plot(hist_v2[\"train\"], label=\"Train\")\n","plt.plot(hist_v2[\"val\"], label=\"Val\")\n","plt.legend(); plt.title(\"SIGNet-v2 Loss\")\n","plt.savefig(f\"{DIRS['fig']}/signet_v2_loss.png\", dpi=300)\n","plt.close()\n","\n","# ============================================================\n","# 12. SAVE MASTER EXCEL\n","# ============================================================\n","\n","excel_path = f\"{DIRS['xls']}/SIGNetV2_MasterResults.xlsx\"\n","with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as w:\n","\n","    # SIGNet\n","    pd.DataFrame(hist_v2).to_excel(w, \"SIGNetV2_Loss\", index=False)\n","    latent_intra_df.to_excel(w, \"Latent_Intra\", index=False)\n","    latent_inter_df.to_excel(w, \"Latent_Inter\", index=False)\n","\n","    # Cluster quality\n","    pd.DataFrame({\n","        \"Silhouette\": [sil],\n","        \"Davies_Bouldin\": [db]\n","    }).to_excel(w, \"Cluster_Quality\", index=False)\n","\n","    # IDInferNet\n","    pd.DataFrame({\"Accuracy\": acc_hist, \"Loss\": loss_hist}).to_excel(w, \"IDInferNet_Ablation\", index=False)\n","    id_report_df.to_excel(w, \"IDInferNet_Report\")\n","    pd.DataFrame(cm_id).to_excel(w, \"IDInferNet_Confusion\")\n","\n","    # DT-GDIN\n","    pd.DataFrame(dt_acc, columns=[\"Accuracy\"]).to_excel(w, \"DTGDIN_Ablation\", index=False)\n","    dt_report_df.to_excel(w, \"DTGDIN_Report\")\n","    pd.DataFrame(cm_dt).to_excel(w, \"DTGDIN_Confusion\")\n","\n","    # Edge + Timing\n","    edge_df.to_excel(w, \"Edge_Metrics\", index=False)\n","    pd.DataFrame([{\n","        \"SIGNetV2_Train_s\": signet_train_time,\n","        \"IDInferNetV2_Train_s\": idinfer_train_time,\n","        \"DTGDIN_V2_Train_s\": dtgdin_train_time\n","    }]).to_excel(w, \"Timing\", index=False)\n","\n","print(\"‚úÖ SIGNet-v2 NOTEBOOK COMPLETE\")\n","print(f\"üìÅ Results saved to: {RES_ROOT}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8HAeCP4xcFvM","executionInfo":{"status":"ok","timestamp":1765991473720,"user_tz":-300,"elapsed":121901,"user":{"displayName":"Community Of Research & Development","userId":"07614819381775789507"}},"outputId":"4614b28c-8ee4-481a-a903-e71cbd905c99"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["‚öôÔ∏è Using device: cuda\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","‚úÖ SIGNet-v2 NOTEBOOK COMPLETE\n","üìÅ Results saved to: /content/drive/MyDrive/DT_Driver_Wise_Data/SIGNetV2_Results\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3494391446.py:499: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(hist_v2).to_excel(w, \"SIGNetV2_Loss\", index=False)\n","/tmp/ipython-input-3494391446.py:500: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  latent_intra_df.to_excel(w, \"Latent_Intra\", index=False)\n","/tmp/ipython-input-3494391446.py:501: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  latent_inter_df.to_excel(w, \"Latent_Inter\", index=False)\n","/tmp/ipython-input-3494391446.py:507: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  }).to_excel(w, \"Cluster_Quality\", index=False)\n","/tmp/ipython-input-3494391446.py:510: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame({\"Accuracy\": acc_hist, \"Loss\": loss_hist}).to_excel(w, \"IDInferNet_Ablation\", index=False)\n","/tmp/ipython-input-3494391446.py:511: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  id_report_df.to_excel(w, \"IDInferNet_Report\")\n","/tmp/ipython-input-3494391446.py:512: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(cm_id).to_excel(w, \"IDInferNet_Confusion\")\n","/tmp/ipython-input-3494391446.py:515: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(dt_acc, columns=[\"Accuracy\"]).to_excel(w, \"DTGDIN_Ablation\", index=False)\n","/tmp/ipython-input-3494391446.py:516: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  dt_report_df.to_excel(w, \"DTGDIN_Report\")\n","/tmp/ipython-input-3494391446.py:517: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  pd.DataFrame(cm_dt).to_excel(w, \"DTGDIN_Confusion\")\n","/tmp/ipython-input-3494391446.py:520: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  edge_df.to_excel(w, \"Edge_Metrics\", index=False)\n","/tmp/ipython-input-3494391446.py:525: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n","  }]).to_excel(w, \"Timing\", index=False)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zbJJJ9b3cFx9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# CVAE_Digital_Twin-v2 | MASTER OPTIMIZATION STAGE (FINAL)\n","# FP32 + Structured Pruning + INT8 Quantization\n","# ARCHITECTURES & DATA PIPELINE EXACTLY PRESERVED\n","# SELF-CONTAINED | REPRODUCIBLE | IEEE-GRADE\n","# ============================================================\n","\n","!pip -q install thop openpyxl\n","\n","# ============================================================\n","# 0. IMPORTS & GLOBALS\n","# ============================================================\n","\n","import os, time, copy\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.utils.prune as prune\n","\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n","from thop import profile\n","from google.colab import drive\n","\n","SEED = 42\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","# ============================================================\n","# 1. PATHS & CONSTANTS\n","# ============================================================\n","\n","drive.mount(\"/content/drive\")\n","\n","DATA_ROOT = \"/content/drive/MyDrive/DT_Driver_Wise_Data\"\n","RES_ROOT  = f\"{DATA_ROOT}/SIGNetV2_Results\"\n","\n","DIRS = {\n","    \"mdl\": f\"{RES_ROOT}/Models\",\n","    \"xls\": f\"{RES_ROOT}/Excel\",\n","}\n","for d in DIRS.values():\n","    os.makedirs(d, exist_ok=True)\n","\n","DRIVERS = [\"B\", \"D\", \"F\"]\n","FEATURES = [\n","    \"Long_Term_Fuel_Trim_Bank1\",\n","    \"Engine_coolant_temperature.1\",\n","    \"Activation_of_Air_compressor\",\n","    \"Torque_of_friction\",\n","    \"Engine_soacking_time\",\n","    \"Intake_air_pressure\",\n","]\n","TIME_COL = \"Time(s)\"\n","\n","# ============================================================\n","# 2. DATA LOADING (ROBUST, FAIL-FAST)\n","# ============================================================\n","\n","dfs = []\n","missing = []\n","\n","for d in DRIVERS:\n","    for s in [\"Train\", \"Valid\"]:\n","        p = f\"{DATA_ROOT}/Driver_{d}_{s}.csv\"\n","        if os.path.exists(p):\n","            df = pd.read_csv(p)\n","            df[\"Driver\"] = d\n","            df[\"Split\"] = s\n","            dfs.append(df)\n","        else:\n","            missing.append(p)\n","\n","if len(dfs) == 0:\n","    raise RuntimeError(\n","        \"No CSV files found. Checked paths:\\n\" + \"\\n\".join(missing)\n","    )\n","\n","df_all = pd.concat(dfs, ignore_index=True)\n","df_all = df_all.dropna(subset=FEATURES + [TIME_COL])\n","\n","print(f\"Loaded {len(df_all)} samples\")\n","\n","# ============================================================\n","# 3. PREPROCESSING (BIT-IDENTICAL TO TRAINING)\n","# ============================================================\n","\n","sc_x = StandardScaler()\n","sc_t = StandardScaler()\n","enc  = OneHotEncoder(sparse_output=False)\n","le   = LabelEncoder()\n","\n","X = sc_x.fit_transform(df_all[FEATURES])\n","T = sc_t.fit_transform(df_all[[TIME_COL]])\n","D = enc.fit_transform(df_all[[\"Driver\"]])\n","C = np.concatenate([T, D], axis=1)\n","\n","X = torch.tensor(X, dtype=torch.float32)\n","C = torch.tensor(C, dtype=torch.float32)\n","\n","mask_va = df_all[\"Split\"] == \"Valid\"\n","X_va = X[mask_va].to(device)\n","C_va = C[mask_va].to(device)\n","drv_va = df_all.loc[mask_va, \"Driver\"].values\n","\n","# ============================================================\n","# 4. MODEL ARCHITECTURES (EXACT, UNCHANGED)\n","# ============================================================\n","\n","class SIGNetV2(nn.Module):\n","    def __init__(self, x_dim, c_dim, z_dim=16):\n","        super().__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(x_dim + c_dim, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","            nn.Linear(256, 128),\n","            nn.ReLU()\n","        )\n","        self.mu = nn.Linear(128, z_dim)\n","        self.lv = nn.Linear(128, z_dim)\n","        self.decoder = nn.Sequential(\n","            nn.Linear(z_dim + c_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, x_dim)\n","        )\n","\n","    def encode(self, x, c):\n","        h = self.encoder(torch.cat([x, c], 1))\n","        return self.mu(h), self.lv(h)\n","\n","    def reparam(self, mu, lv):\n","        return mu + torch.randn_like(mu) * torch.exp(0.5 * lv)\n","\n","    def decode(self, z, c):\n","        return self.decoder(torch.cat([z, c], 1))\n","\n","    def forward(self, x, c):\n","        mu, lv = self.encode(x, c)\n","        z = self.reparam(mu, lv)\n","        return self.decode(z, c), mu, lv\n","\n","\n","class IDInferNetV2(nn.Module):\n","    def __init__(self, z_dim, n):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(z_dim, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(128, n)\n","        )\n","    def forward(self, z): return self.net(z)\n","\n","\n","class DTGDINV2(nn.Module):\n","    def __init__(self, d, n):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(d, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, n)\n","        )\n","    def forward(self, x): return self.net(x)\n","\n","# ============================================================\n","# 5. LOAD TRAINED WEIGHTS\n","# ============================================================\n","\n","x_dim = X_va.shape[1]\n","c_dim = C_va.shape[1]\n","z_dim = 16\n","n_cls = len(DRIVERS)\n","\n","signet = SIGNetV2(x_dim, c_dim).to(device)\n","signet.load_state_dict(torch.load(f\"{DIRS['mdl']}/signet_v2.pt\"))\n","signet.eval()\n","\n","with torch.no_grad():\n","    Z_va = signet.encode(X_va, C_va)[0]\n","\n","idnet = IDInferNetV2(z_dim, n_cls).to(device)\n","idnet.load_state_dict(torch.load(f\"{DIRS['mdl']}/idinfernet_v2.pt\"))\n","idnet.eval()\n","\n","dtgdin = DTGDINV2(x_dim, n_cls).to(device)\n","dtgdin.load_state_dict(torch.load(f\"{DIRS['mdl']}/dtgdin_v2.pt\"))\n","dtgdin.eval()\n","\n","# ============================================================\n","# 6. EDGE PROFILING (QUANT-SAFE)\n","# ============================================================\n","\n","def edge_profile(model, inputs, dtype_bytes, runs=50):\n","    model.eval()\n","    try:\n","        dev = next(model.parameters()).device\n","    except StopIteration:\n","        dev = torch.device(\"cpu\")\n","\n","    if isinstance(inputs, tuple):\n","        inputs = tuple(i.to(dev) for i in inputs)\n","    else:\n","        inputs = inputs.to(dev)\n","\n","    with torch.no_grad():\n","        flops, params = profile(model, inputs=inputs, verbose=False)\n","        t0 = time.time()\n","        for _ in range(runs):\n","            _ = model(*inputs) if isinstance(inputs, tuple) else model(inputs)\n","        latency = (time.time() - t0) / runs * 1000\n","\n","    return {\n","        \"Params\": int(params),\n","        \"FLOPs\": int(flops),\n","        \"Latency_ms\": latency,\n","        \"Memory_KB\": params * dtype_bytes / 1024,\n","        \"Energy_mJ_est\": flops * 3e-9\n","    }\n","\n","# ============================================================\n","# 7. OPTIMIZATION OPERATORS\n","# ============================================================\n","\n","def prune_model(model, amount=0.3):\n","    m = copy.deepcopy(model)\n","    for l in m.modules():\n","        if isinstance(l, nn.Linear):\n","            prune.ln_structured(l, \"weight\", amount, n=1, dim=0)\n","            prune.remove(l, \"weight\")\n","    return m\n","\n","def quant_model(model):\n","    return torch.quantization.quantize_dynamic(\n","        model.cpu(), {nn.Linear}, dtype=torch.qint8\n","    )\n","\n","# ============================================================\n","# 8. OPTIMIZED MODELS\n","# ============================================================\n","\n","models = {\n","    \"SIGNetV2_FP32\": signet,\n","    \"SIGNetV2_Pruned\": prune_model(signet),\n","    \"SIGNetV2_Quant\": quant_model(signet),\n","\n","    \"IDInferNetV2_FP32\": idnet,\n","    \"IDInferNetV2_Pruned\": prune_model(idnet),\n","    \"IDInferNetV2_Quant\": quant_model(idnet),\n","\n","    \"DTGDINV2_FP32\": dtgdin,\n","    \"DTGDINV2_Pruned\": prune_model(dtgdin),\n","    \"DTGDINV2_Quant\": quant_model(dtgdin),\n","}\n","\n","# ============================================================\n","# 9. EDGE METRICS (REAL DATA)\n","# ============================================================\n","\n","rows = []\n","\n","for name, model in models.items():\n","    is_quant = \"Quant\" in name\n","    dtype_bytes = 1 if is_quant else 4\n","\n","    if \"SIGNetV2\" in name:\n","        inputs = (X_va[:1], C_va[:1])\n","    elif \"IDInferNetV2\" in name:\n","        inputs = (Z_va[:1],)\n","    else:\n","        inputs = (X_va[:1],)\n","\n","    rows.append({\n","        \"Model\": name,\n","        **edge_profile(model, inputs, dtype_bytes)\n","    })\n","\n","    torch.save(model.state_dict(), f\"{DIRS['mdl']}/{name}.pt\")\n","\n","edge_df = pd.DataFrame(rows)\n","\n","# ============================================================\n","# 10. SAVE RESULTS\n","# ============================================================\n","\n","out_xls = f\"{DIRS['xls']}/SIGNetV2_Edge_Optimization.xlsx\"\n","edge_df.to_excel(out_xls, index=False)\n","\n","print(\"MASTER OPTIMIZATION COMPLETE\")\n","print(\"Models:\", DIRS[\"mdl\"])\n","print(\"Excel:\", out_xls)\n"],"metadata":{"id":"3lp1Q_QVcF1F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765991487080,"user_tz":-300,"elapsed":13377,"user":{"displayName":"Community Of Research & Development","userId":"07614819381775789507"}},"outputId":"f0a5ad0b-b342-4d39-bfba-a4270945f8d3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Loaded 95368 samples\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1412723234.py:240: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n","For migrations of users: \n","1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n","2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n","3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n","see https://github.com/pytorch/ao/issues/2259 for more details\n","  return torch.quantization.quantize_dynamic(\n"]},{"output_type":"stream","name":"stdout","text":["MASTER OPTIMIZATION COMPLETE\n","Models: /content/drive/MyDrive/DT_Driver_Wise_Data/SIGNetV2_Results/Models\n","Excel: /content/drive/MyDrive/DT_Driver_Wise_Data/SIGNetV2_Results/Excel/SIGNetV2_Edge_Optimization.xlsx\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"tQ1pe4e1Oidw"},"execution_count":null,"outputs":[]}]}